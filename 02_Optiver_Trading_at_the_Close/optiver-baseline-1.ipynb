{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":144941346,"sourceType":"kernelVersion"},{"sourceId":150360854,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. nan들을 fillna로 해결하는 코드가 있는지 (effective한지 체크)<br>\n2. MACD, RSI, STD< SKEW 등의 features 넣어주긔<br>\n3. 모델 ensembling / stacking / NN 테스트","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport joblib\nimport numpy as np\n%load_ext cudf.pandas      # pandas speed up\nimport pandas as pd \n# pd.set_option(\"display.max_columns\", 50)\n# pd.set_option(\"display.max_rows\", 50)\n\nfrom warnings import filterwarnings\nfilterwarnings(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pd.options.mode.chained_assignment = None # pd 경고문을 안 뜨게 해\n\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\n\nimport json\nfrom itertools import combinations\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nPATH = \"/kaggle/input/optiver-trading-at-the-close/\"\n\n# median_vol = pd.read_csv(\"/kaggle/input/optiver-memoryreduceddatasets/MedianVolV2.csv\")\n# median_vol.index.name = \"stock_id\"\n# median_vol = median_vol[[\"overall_medvol\",\"first5min_medvol\",\"last5min_medvol\"]]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-21T00:11:39.287240Z","iopub.execute_input":"2023-12-21T00:11:39.287531Z","iopub.status.idle":"2023-12-21T00:11:44.496180Z","shell.execute_reply.started":"2023-12-21T00:11:39.287502Z","shell.execute_reply":"2023-12-21T00:11:44.495138Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"change_dtypes = {\"stock_id\": np.uint8,\n                \"date_id\": np.uint16, \n                \"seconds_in_bucket\": np.uint16,\n                \"imbalance_buy_sell_flag\": np.int8,\n                \"time_id\": np.uint16}\n\n# train_df = pd.read_csv(PATH + \"train.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\ntrain_df2 = pd.read_parquet(\"/kaggle/input/optiver-datapreparation/XTrainIntCmp.parquet\")\ny = pd.read_parquet(\"/kaggle/input/optiver-datapreparation/Ytrain.parquet\")\n# test_df = pd.read_csv(PATH + \"example_test_files/test.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n\n# train_df2.dropna(subset=[\"target\"], inplace=True)\n# train_df2.reset_index(drop=True, inplace=True)\n\ndel change_dtypes","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:44.498241Z","iopub.execute_input":"2023-12-21T00:11:44.498570Z","iopub.status.idle":"2023-12-21T00:11:47.671312Z","shell.execute_reply.started":"2023-12-21T00:11:44.498532Z","shell.execute_reply":"2023-12-21T00:11:47.670520Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df2[\"far_price\"] = train_df2[\"far_price\"][train_df2[\"far_price\"].isna() & train_df2[\"date_id\"] < 300].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:47.672396Z","iopub.execute_input":"2023-12-21T00:11:47.672689Z","iopub.status.idle":"2023-12-21T00:11:47.787368Z","shell.execute_reply.started":"2023-12-21T00:11:47.672663Z","shell.execute_reply":"2023-12-21T00:11:47.786335Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df2[\"near_price\"] = train_df2[\"near_price\"][train_df2[\"near_price\"].isna() & train_df2[\"date_id\"] < 300].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:47.789413Z","iopub.execute_input":"2023-12-21T00:11:47.789713Z","iopub.status.idle":"2023-12-21T00:11:47.893064Z","shell.execute_reply.started":"2023-12-21T00:11:47.789687Z","shell.execute_reply":"2023-12-21T00:11:47.892105Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print(f\"starting memory: {start_mem}\")\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # check if the column's data type is not \"object\" (i.e. numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                    \n                else:\n                    # check if the column's data type is a float\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float32)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float32)\n                        \n        if verbose:\n            print(f\"start memory: {start_mem}\")\n            end_mem = df.memory_usage().sum() / 1024 ** 2\n            print(f\"current memory: {end_mem}\")\n        \n        return df","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:47.902176Z","iopub.execute_input":"2023-12-21T00:11:47.902498Z","iopub.status.idle":"2023-12-21T00:11:47.915165Z","shell.execute_reply.started":"2023-12-21T00:11:47.902454Z","shell.execute_reply":"2023-12-21T00:11:47.914275Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n    for i in prange(num_combinations):\n        a,b,c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n            \n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a,b,c in combinations(price, 3)]\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n    columns = [f\"{a}_{b}_{c}_imb2\" for a,b,c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:47.916298Z","iopub.execute_input":"2023-12-21T00:11:47.916846Z","iopub.status.idle":"2023-12-21T00:11:48.693132Z","shell.execute_reply.started":"2023-12-21T00:11:47.916821Z","shell.execute_reply":"2023-12-21T00:11:48.692076Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/chinzorigtganbat/feature-generation-technical-analysis-functions\n@njit(fastmath=True)\ndef rolling_average(arr, window):\n    n = len(arr)\n    result = np.empty(n)\n    result[:window] = np.nan\n    cumsum = np.cumsum(arr)\n    \n    for i in range(window, n):\n        result[i] = (cumsum[i] - cumsum[i-window]) / window\n    return result\n\n\n@njit(parallel=True)\ndef compute_rolling_averages(df_values, window_sizes):\n    num_rows, num_features = df_values.shape\n    num_windows = len(window_sizes)\n    rolling_features = np.empty((num_rows, num_features, num_windows))\n    \n    for feature_idx in prange(num_features):\n        for window_idx, window in enumerate(window_sizes):\n            rolling_features[:, feature_idx, window_idx] = rolling_average(df_values[:, feature_idx], window)\n    return rolling_features\n\n\n@njit(parallel=True)\ndef calculate_rsi(prices, period=14):\n    rsi_values = np.zeros_like(prices)\n    \n    for col in prange(prices.shape[1]):\n        price_data = prices[:, col]\n        delta = np.zeros_like(price_data)\n        delta[1:] = price_data[1:] - price_data[:-1]\n        gain = np.where(delta > 0, delta, 0)\n        loss = np.where(delta < 0, -delta, 0)\n        \n        avg_gain = np.mean(gain[:period])\n        avg_loss = np.mean(loss[:period])\n        \n        if avg_loss != 0:\n            rs = avg_gain / avg_loss\n        else:\n            rs = 1e-9\n            \n        rsi_values[:period, col] = 100 - (100 / (1+rs))\n        \n        for i in prange(period-1, len(price_data)-1):\n            avg_gain = (avg_gain * (period - 1) + gain[i]) / period\n            avg_loss = (avg_loss * (period - 1) + loss[i]) / period\n            \n            if avg_loss != 0:\n                rs = avg_gain / avg_loss\n            else:\n                rs = 1e-9\n            \n            rsi_values[i+1, col] = 100 - (100 / (1 + rs))\n    return rsi_values\n\n\n@njit(parallel=True)\ndef calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n    rows, cols = data.shape\n    macd_values = np.empty((rows, cols))\n    signal_line_values = np.empty((rows, cols))\n    histogram_values = np.empty((rows, cols))\n    \n    for i in prange(cols):\n        short_ema = np.zeros(rows)\n        long_ema = np.zeros(rows)\n        for j in range(1, rows):\n            short_ema[j] = (data[j, i] - short_ema[j - 1]) * (2 / (short_window + 1)) + short_ema[j - 1]\n            long_ema[j] = (data[j, i] - long_ema[j - 1]) * (2 / (long_window + 1)) + long_ema[j - 1]\n            \n        macd_values[:, i] = short_ema - long_ema\n        signal_line = np.zeros(rows)\n        \n        for j in range(1, rows):\n            signal_line[j] = (macd_values[j,i] - signal_line[j - 1]) * (2 / (signal_window + 1)) + signal_line[j - 1]\n            \n        signal_line_values[:, i] = signal_line\n        histogram_values[:, i] = macd_values[:, i] - signal_line\n    return macd_values, signal_line_values, histogram_values\n\n\n\n@njit(parallel=True)\ndef calculate_bband(data, window=20, num_std_dev=2):\n    num_rows, num_cols = data.shape\n    upper_bands = np.zeros_like(data)\n    lower_bands = np.zeros_like(data)\n    mid_bands = np.zeros_like(data)\n    \n    for col in prange(num_cols):\n        for i in prange(window-1, num_rows):\n            window_slice = data[i-window+1:i+1, col]\n            mid_bands[i, col] = np.mean(window_slice)\n            std_dev = np.std(window_slice)\n            upper_bands[i, col] = mid_bands[i, col] + num_std_dev * std_dev\n            lower_bands[i, col] = mid_bands[i, col] - num_std_dev * std_dev\n            \n    return upper_bands, mid_bands, lower_bands","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:48.694864Z","iopub.execute_input":"2023-12-21T00:11:48.695430Z","iopub.status.idle":"2023-12-21T00:11:48.726799Z","shell.execute_reply.started":"2023-12-21T00:11:48.695392Z","shell.execute_reply":"2023-12-21T00:11:48.725942Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def generate_ta(df):\n    # define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    \n    for stock_id, values in df.groupby([\"stock_id\"])[prices]:\n        # RSI: relative strength index\n        col_rsi = [f\"rsi_{col}\" for col in values.columns]\n        rsi_values = calculate_rsi(values.values)\n        df.loc[values.index, col_rsi] = rsi_values\n        gc.collect()\n        \n        # MACD\n        macd_values, signal_line_values, histogram_values = calculate_macd(values.values)\n        col_macd = [f\"macd_{col}\" for col in values.columns]\n        col_signal = [f\"macd_sig_{col}\" for col in values.columns]\n        col_hist = [f\"macd_hist_{col}\" for col in values.columns]\n        \n        df.loc[values.index, col_macd] = macd_values\n        df.loc[values.index, col_signal] = signal_line_values\n        df.loc[values.index, col_hist] = histogram_values\n        gc.collect()\n        \n        # bollinger Bands\n        bband_upper_values, bband_mid_values, bband_lower_values = calculate_bband(values.values, window=20, num_std_dev=2)\n        col_bband_upper = [f\"bband_upper_{col}\" for col in values.columns]\n        col_bband_mid = [f\"bband_mid_{col}\" for col in values.columns]\n        col_bband_lower = [f\"bband_lower_{col}\" for col in values.columns]\n        \n        df.loc[values.index, col_bband_upper] = bband_upper_values\n        df.loc[values.index, col_bband_mid] = bband_mid_values\n        df.loc[values.index, col_bband_lower] = bband_lower_values\n        gc.collect()\n        \n    return df, [col for col in df.columns if col not in [\"date_id\",\"stock_id\"]]","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:48.727933Z","iopub.execute_input":"2023-12-21T00:11:48.728188Z","iopub.status.idle":"2023-12-21T00:11:48.741214Z","shell.execute_reply.started":"2023-12-21T00:11:48.728165Z","shell.execute_reply":"2023-12-21T00:11:48.740329Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def create_features(df, reduce_memory=True):\n#     cols_to_drop = [\"imbalance_buy_sell_flag\"]\n    prices = [\"reference_price\",\"far_price\",\"near_price\",\"ask_price\",\"bid_price\",\"wap\"]\n    sizes = [\"matched_size\",\"bid_size\",\"ask_size\",\"imbalance_size\"]\n    \n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n#     df[\"mid_price\"] = df.eval(\"(ask_price + bid_size) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size - ask_size)/(bid_size + ask_size)\")\n#     df[\"matched_imbalance\"] = df.eval(\"(imbalance_size - matched_size)/(matched_size + imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    df[\"price_spread\"] = df.eval(\"ask_price - bid_price\")\n    df[\"imbalance_ratio\"] = df.eval(\"imbalance_size / matched_size\")\n    \n    df[\"ask_volume\"] = df.eval(\"ask_size * ask_price\")\n    df[\"bid_volume\"] = df.eval(\"bid_size * bid_price\")\n    \n    df[\"ask_bid_volumes_diff\"] = df[\"ask_volume\"] - df[\"bid_volume\"]\n    \n    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"]) # size imbalance\n    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n    \n    df[\"imbalance_buy_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==1, True, False)\n    df[\"imbalance_sell_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==-1, True, False)\n    \n    # create features for pairwise price imbalances\n    # https://www.kaggle.com/code/judith007/lb-5-3393-rapids-gpu-speeds-up-feature-engineer\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n    \n    # V2 features # 아래 피쳐들 의미를 잘 모르겠땅\n    df[\"imbalance_momentum\"] = df.groupby([\"stock_id\"])[\"imbalance_size\"].diff(periods=1)/df[\"matched_size\"]    \n    df[\"spread_intensity\"] = df.groupby([\"stock_id\"])[\"price_spread\"].diff()\n    df[\"price_pressure\"] = df[\"imbalance_size\"] * (df[\"ask_price\"] - df[\"bid_price\"])\n    df[\"market_urgency\"] = df[\"price_spread\"] * df[\"liquidity_imbalance\"]\n    df[\"depth_pressure\"] = (df[\"ask_size\"] - df[\"bid_size\"]) * (df[\"far_price\"] - df[\"near_price\"])\n    \n    # V3 features\n    for col in [\"matched_size\",\"imbalance_size\",\"reference_price\",\"imbalance_buy_sell_flag\"]:\n        for window in [1,2,3,10]:      # 이거 숫자들의 의미가 뭐지?\n            df[f\"{col}_shift_{window}\"] = df.groupby(\"stock_id\")[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby(\"stock_id\")[col].pct_change(window)\n            \n    # calculate diff features for specific columns\n    for col in [\"ask_price\",\"bid_price\",\"ask_size\",\"bid_size\"]:\n        for window in [1,2,3,10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n            \n    for c in [[\"ask_price\",\"bid_price\",\"wap\",\"reference_price\"], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    \n    if reduce_memory:\n        print(f\"Reducing memory usage... (Current: {df.memory_usage().sum() / 1024 / 1024:.1f} MB)\")\n        df = df.astype({\"stock_id\":\"uint8\",\n                       \"seconds_in_bucket\":\"uint16\",\n                       # converts all float64s to float32s\n                       **{col:\"float32\" for col in df.columns if df[col].dtype == \"float64\"}})\n        print(f\"Memory usage after reduction: {df.memory_usage().sum() / 1024 / 1024:.1f} MB\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:48.745089Z","iopub.execute_input":"2023-12-21T00:11:48.745791Z","iopub.status.idle":"2023-12-21T00:11:48.767288Z","shell.execute_reply.started":"2023-12-21T00:11:48.745758Z","shell.execute_reply":"2023-12-21T00:11:48.766365Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"train_df2 = create_features(train_df2)\ntrain_df2, ta_cols = generate_ta(train_df2)\nta_cols = [x for x in ta_cols if (x != \"target\")]\ntrain_df2 = reduce_mem_usage(train_df2)\ndate_ids = train_df2[\"date_id\"].values\n\ntrain_df2.drop(columns=[\"date_id\",\"stock_id\"\n                       # ,\"target\"\n                       ], inplace=True)\n\n# feat_importances = [\"seconds_in_bucket\",\"market_urgency\",\"near_price_ask_price_imb\",\\\n# \"reference_price_wap_imb\",\"wap\",\"far_price_bid_price_imb\",\"reference_price_bid_price_imb\",\\\n# \"ask_price_diff_2\",\"imbalance_size_shift_1\",\"far_price_near_price_imb\",\\\n# \"matched_size_ask_size_imbalance_size_imb2\",\\\n# \"matched_size_ret_10\",\"price_spread\",\"matched_size_shift_10\",\"bid_price_diff_1\",\"spread_intensity\",\\\n# \"ask_price_bid_price_reference_price_imb2\",\"reference_price_ask_price_imb\",\\\n# \"matched_size_bid_size_imbalance_size_imb2\",\n# \"reference_price_ret_3\",\"far_price_ask_price_imb\",\\\n#                     \"imbalance_momentum\",\"bid_price\",\"ask_price\",\\\n# \"ask_price_diff_1\",\"imbalance_ratio\",\"imbalance_buy_sell_flag\",\\\n#                     \"reference_price_ret_10\",\\\n                    \n# \"reference_price\",\"volume\",\"bid_size_ask_size_imbalance_size_imb2\",\"imbalance_size\",\\\n# \"bid_volume\",\"matched_size_shift_1\",\"matched_size_bid_size_ask_size_imb2\",\\\n                    \n# \"bid_price_over_ask_price\",\"bid_price_wap_imb\",\"reference_price_shift_3\",\\\n# \"reference_price_ret_1\",\"bid_size_diff_1\",\"reference_price_shift_1\",\\\n#                     \"ask_size\",\"bid_price_diff_10\",\\\n# \"matched_size_ret_1\",\"matched_size_shift_2\",\"matched_size_shift_3\",\\\n#                     \"ask_price_diff_10\",\\\n# \"ask_size_diff_1\",\"ask_volume\",\"ask_price_bid_price_wap_imb2\",\\\n#                     \"reference_price_ret_2\",\"reference_price_shift_2\",\\\n# \"matched_size_ret_3\",\"bid_size_diff_3\",\"matched_size_ret_2\",\\\n#                     \"price_pressure\",\"bid_size_diff_2\",\\\n# \"ask_size_diff_3\",\"reference_price_far_price_imb\",\\\n#                     \"liquidity_imbalance\",\"bid_price_wap_reference_price_imb2\",\\\n# \"bid_size_diff_10\",\"ask_size_diff_10\",\\\n#                 \"bid_size_over_ask_size\",\"ask_bid_volumes_diff\",\\\n# \"depth_pressure\"\n#                    ] + ta_cols #, \"overall_medvol\",\"target\"\n\n\ntrain_df2 = train_df2[ta_cols]\n# del feat_importances\ngc.collect()\n\n# import seaborn as sns\n# sns.heatmap(np.around(train_df.corr(), 2)) #, annot=True) ","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:11:48.768464Z","iopub.execute_input":"2023-12-21T00:11:48.768777Z","iopub.status.idle":"2023-12-21T00:14:39.099720Z","shell.execute_reply.started":"2023-12-21T00:11:48.768749Z","shell.execute_reply":"2023-12-21T00:14:39.098616Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \u001b[1m\nEncountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'comb_indices' of function 'compute_triplet_imbalance'.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n\u001b[1m\nFile \"../../tmp/ipykernel_33/3417327308.py\", line 3:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n/opt/conda/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \u001b[1m\nEncountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'comb__indices' of function '__numba_parfor_gufunc_0x7e6d340ab340'.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n\u001b[1m\nFile \"<string>\", line 1:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n","output_type":"stream"},{"name":"stdout","text":"Reducing memory usage... (Current: 3926.3 MB)\nMemory usage after reduction: 2008.1 MB\nstarting memory: 3815.4894256591797\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# # IR: Information Ratio     as a feature..넣어주긔 \n# def calculate_information_ratio(x1, x2):\n#     assert len(x1) == len(x2)\n#     interval = 495000  # 45 days","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:39.101207Z","iopub.execute_input":"2023-12-21T00:14:39.101877Z","iopub.status.idle":"2023-12-21T00:14:39.105891Z","shell.execute_reply.started":"2023-12-21T00:14:39.101842Z","shell.execute_reply":"2023-12-21T00:14:39.105008Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lgb_params2 = {'objective': 'mae',\n 'n_estimators': 5500,\n 'num_leaves': 128,\n 'subsample': 0.6,\n 'colsample_bytree': 0.8,\n 'learning_rate': 5e-05,\n 'max_depth': 11,\n 'n_jobs': 4,\n 'device': 'gpu',\n 'verbosity': -1,\n 'importance_type': 'gain'}\n\n# lgb_params3 = {\n#     \"objective\": \"mae\",\n#     \"n_estimators\": 6000,\n#     \"num_leaves\": 256,\n#     \"subsample\": 0.6,\n#     'learning_rate': 0.00871, \n#     'max_depth': 11,\n#     \"colsample_bytree\" : 0.8,\n#     \"n_jobs\": 4,\n#     \"device\": \"gpu\",\n#     \"verbosity\": -1,\n#     \"importance_type\": \"gain\",\n#     \"random_state\": 42,\n#     \"max_bin\": 247\n# }\n\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:39.107042Z","iopub.execute_input":"2023-12-21T00:14:39.107301Z","iopub.status.idle":"2023-12-21T00:14:39.265945Z","shell.execute_reply.started":"2023-12-21T00:14:39.107278Z","shell.execute_reply":"2023-12-21T00:14:39.264963Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"# https://www.kaggle.com/code/sunghoshim/optiver-checking-feature-importance\n# model = lgb.Booster(model_file='/kaggle/input/explained-singel-model-optiver/modelitos_para_despues/doblez-conjunto.txt')\n# importances = model.feature_importance(importance_type=\"gain\")\n# feature_names = model.feature_name()\n\n# ser = pd.Series(importances, index=feature_names)\n# del importances, feature_names\n# ser_sorted = ser.sort_values().astype(int)\n\n# import matplotlib.pyplot as plt\n# max_num_features = 177\n# fig, ax = plt.subplots(figsize=(12, 15))\n# bars = ax.barh(ser_sorted.index[-max_num_features:],\n#               ser_sorted[-max_num_features:])\n# ax.bar_label(bars, padding=5, fmt=\"{:,.0f}\")\n# del model\n# gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:39.266883Z","iopub.execute_input":"2023-12-21T00:14:39.267175Z","iopub.status.idle":"2023-12-21T00:14:39.278151Z","shell.execute_reply.started":"2023-12-21T00:14:39.267150Z","shell.execute_reply":"2023-12-21T00:14:39.277343Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# important_features = ser_sorted.index[-80:]\n# important_features = list(important_features)","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:39.279329Z","iopub.execute_input":"2023-12-21T00:14:39.279609Z","iopub.status.idle":"2023-12-21T00:14:39.288229Z","shell.execute_reply.started":"2023-12-21T00:14:39.279586Z","shell.execute_reply":"2023-12-21T00:14:39.287514Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"feats = ['ask_bid_volumes_diff', 'ask_price', 'ask_price_bid_price_imb', 'ask_price_bid_price_reference_price_imb2', \n         'ask_price_bid_price_wap_imb2', 'ask_price_diff_1', 'ask_price_diff_10', 'ask_price_diff_2', 'ask_price_diff_3', \n         'ask_price_wap_imb', 'ask_price_wap_reference_price_imb2', 'ask_size', 'ask_size_diff_1', 'ask_size_diff_10',\n         'ask_size_diff_2', 'ask_size_diff_3', 'ask_volume', 'bband_lower_ask_price', 'bband_lower_bid_price', 'bband_lower_far_price', \n         'bband_lower_near_price', 'bband_lower_reference_price', 'bband_lower_wap', 'bband_mid_ask_price', 'bband_mid_bid_price', \n         'bband_mid_far_price', 'bband_mid_near_price', 'bband_mid_reference_price', 'bband_mid_wap', 'bband_upper_ask_price', 'bband_upper_bid_price', 'bband_upper_far_price', 'bband_upper_near_price', 'bband_upper_reference_price', 'bband_upper_wap', 'bid_price', 'bid_price_diff_1', 'bid_price_diff_10', 'bid_price_diff_2', 'bid_price_diff_3', 'bid_price_over_ask_price', 'bid_price_wap_imb', 'bid_price_wap_reference_price_imb2', 'bid_size', 'bid_size_ask_size_imbalance_size_imb2', 'bid_size_diff_1', 'bid_size_diff_10', 'bid_size_diff_2', 'bid_size_diff_3', 'bid_size_over_ask_size', 'bid_volume', 'depth_pressure', 'far_price', 'far_price_ask_price_imb', 'far_price_bid_price_imb', 'far_price_near_price_imb', 'far_price_wap_imb', 'imbalance_buy_flag', 'imbalance_buy_sell_flag', 'imbalance_buy_sell_flag_ret_1', 'imbalance_buy_sell_flag_ret_10', 'imbalance_buy_sell_flag_ret_2', 'imbalance_buy_sell_flag_ret_3', 'imbalance_buy_sell_flag_shift_1', 'imbalance_buy_sell_flag_shift_10', 'imbalance_buy_sell_flag_shift_2', 'imbalance_buy_sell_flag_shift_3', 'imbalance_momentum', 'imbalance_ratio', 'imbalance_sell_flag', 'imbalance_size', 'imbalance_size_ret_1', 'imbalance_size_ret_10', 'imbalance_size_ret_2', 'imbalance_size_ret_3', 'imbalance_size_shift_1', 'imbalance_size_shift_10', 'imbalance_size_shift_2', 'imbalance_size_shift_3', 'liquidity_imbalance', 'macd_ask_price', 'macd_bid_price', 'macd_far_price', 'macd_hist_ask_price', 'macd_hist_bid_price', 'macd_hist_far_price', 'macd_hist_near_price', 'macd_hist_reference_price', 'macd_hist_wap', 'macd_near_price', 'macd_reference_price', 'macd_sig_ask_price', 'macd_sig_bid_price', 'macd_sig_far_price', 'macd_sig_near_price', 'macd_sig_reference_price', 'macd_sig_wap', 'macd_wap', 'market_urgency', 'matched_size', 'matched_size_ask_size_imbalance_size_imb2', 'matched_size_bid_size_ask_size_imb2', 'matched_size_bid_size_imbalance_size_imb2', 'matched_size_ret_1', 'matched_size_ret_10', 'matched_size_ret_2', 'matched_size_ret_3', 'matched_size_shift_1', 'matched_size_shift_10', 'matched_size_shift_2', 'matched_size_shift_3', 'near_price', 'near_price_ask_price_imb', 'near_price_bid_price_imb', 'near_price_wap_imb', 'price_pressure', 'price_spread', 'reference_price', 'reference_price_ask_price_imb', 'reference_price_bid_price_imb', 'reference_price_far_price_imb', 'reference_price_near_price_imb', 'reference_price_ret_1', 'reference_price_ret_10', 'reference_price_ret_2', 'reference_price_ret_3', 'reference_price_shift_1', 'reference_price_shift_10', 'reference_price_shift_2', 'reference_price_shift_3', 'reference_price_wap_imb', 'rsi_ask_price', 'rsi_bid_price', 'rsi_far_price', 'rsi_near_price', 'rsi_reference_price', 'rsi_wap', 'seconds_in_bucket', 'size_imbalance', 'spread_intensity', 'volume', 'wap']\nfeats = [f for f in feats if (f in train_df2.columns)]\nfeats = list(np.unique(feats))","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:39.290079Z","iopub.execute_input":"2023-12-21T00:14:39.290337Z","iopub.status.idle":"2023-12-21T00:14:39.301625Z","shell.execute_reply.started":"2023-12-21T00:14:39.290307Z","shell.execute_reply":"2023-12-21T00:14:39.300757Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"train_df2 = train_df2.loc[:, ~train_df2.columns.duplicated()].copy()\nfeature_name = feats\n# del important_features, ser_sorted, \ndel ta_cols, feats\n# train_df.dropna(inplace=True)\n# train_df = train_df.reset_index()\ngc.collect()\n\n# print(train_df2.shape)\n# print(len(y))","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:39.302698Z","iopub.execute_input":"2023-12-21T00:14:39.302927Z","iopub.status.idle":"2023-12-21T00:14:46.005125Z","shell.execute_reply.started":"2023-12-21T00:14:39.302906Z","shell.execute_reply":"2023-12-21T00:14:46.004174Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]},{"cell_type":"code","source":"import xgboost as xgb\nimport catboost as cbt\n\nn_fold = 5\nindex = np.arange(len(train_df2))\nmodel_dict = {\"lgb\": lgb.LGBMRegressor(objective=\"regression_l1\",\n                                     n_estimators=500),\n              \"xgb\": xgb.XGBRegressor(tree_method=\"hist\",\n                                     objective=\"reg:absoluteerror\",\n                                     n_estimator=500),\n              \"cbt\": cbt.CatBoostRegressor(objective=\"MAE\",\n                                          iterations=3000)\n             }\nis_training = True\ndef train(model_dict, modelname=\"lgb\"):\n    if is_training:\n        model = model_dict[modelname]\n        model.fit(train_df2[index%n_fold != i], y[index%n_fold != i],\n                 eval_set=[(train_df2[index%n_fold == i], y[index%n_fold == i])],\n                 verbose=10, early_stopping_rounds=100)\n        model.append(model)\n        joblib.dump(model, f\"/kaggle/working/{modelname}.model\")\n        \n    \nfor i in range(n_fold):\n    train(model_dict, \"lgb\")","metadata":{"execution":{"iopub.status.busy":"2023-12-21T00:14:46.008306Z","iopub.execute_input":"2023-12-21T00:14:46.008904Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:726: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n/opt/conda/lib/python3.10/site-packages/lightgbm/sklearn.py:736: UserWarning: 'verbose' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n  _log_warning(\"'verbose' argument is deprecated and will be removed in a future release of LightGBM. \"\n","output_type":"stream"}]},{"cell_type":"code","source":"# models = []\n# scores = []\n\n# num_folds = 10\n# fold_size = 480 // num_folds\n\n# for i in range(num_folds):\n#     start = i * fold_size\n#     end = start + fold_size\n    \n#     # Define the purged set ranges\n#     purged_before_start = start - 2\n#     purged_before_end = start + 2\n#     purged_after_start = end - 2\n#     purged_after_end = end + 2\n    \n#     # Exclude the purged ranges from the test set\n#     purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n#                  ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n    \n#     # Define test_indices excluding the purged set\n#     test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n#     train_indices = ~test_indices & ~purged_set\n    \n#     df_fold_train = train_df2[train_indices]\n#     df_fold_train_target = y[train_indices]\n#     df_fold_valid = train_df2[test_indices]\n#     df_fold_valid_target = y[test_indices]\n\n#     print(f\"Fold {i+1} Model Training\")\n    \n#     # Train a LightGBM model for the current fold\n#     lgb_model = lgb.LGBMRegressor(**lgb_params2)\n#     lgb_model.fit(\n#         df_fold_train[train_df2.columns],\n#         df_fold_train_target,\n#         eval_set=[(df_fold_valid[train_df2.columns], \n#                    df_fold_valid_target)],\n#         callbacks=[\n#             lgb.callback.early_stopping(stopping_rounds=100),\n#             lgb.callback.log_evaluation(period=100),\n#         ],\n#     )\n\n#     # Append the model to the list\n#     models.append(lgb_model)\n#     # Save the model to a file\n# #     model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n# #     lgb_model.booster_.save_model(model_filename)\n# #     print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n#     # Evaluate model performance on the validation set\n#     fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n#     fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n#     scores.append(fold_score)\n#     print(f\"Fold {i+1} MAE: {fold_score}\")\n\n#     # Free up memory by deleting fold specific variables\n#     del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n#     gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\n\nlgb_model.fit(train_df2, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = 0\nfor (test, revealed_targets, sample_predict) in iter_test:\n    sample_predict[\"target\"] = lgbmodel.predict(test.drop(\"row_id\", axis=1))\n    env.predict(sample_predict)\n    cnt += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{},"execution_count":null,"outputs":[]}]}