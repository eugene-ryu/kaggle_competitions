{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":150360854,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"1. nan들을 fillna로 해결하는 코드가 있는지 (effective한지 체크)<br>\n2. MACD, RSI, STD< SKEW 등의 features 넣어주긔<br>\n3. 모델 ensembling / stacking / NN 테스트","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport numpy as np\nimport pandas as pd \n# pd.set_option(\"display.max_columns\", 50)\n# pd.set_option(\"display.max_rows\", 50)\n\nfrom warnings import filterwarnings\nfilterwarnings(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pd.options.mode.chained_assignment = None # pd 경고문을 안 뜨게 해\n\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport joblib\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\n\nimport json\nimport optuna\nfrom itertools import combinations\n# from pprint import pprint, pformat\n# from optuna.visualization import (plot_optimization_history,\n#                                  plot_param_importances,\n#                                  plot_parallel_coordinate)\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nPATH = \"/kaggle/input/optiver-trading-at-the-close/\"\n# median_vol = pd.read_csv(\"/kaggle/input/optiver-memoryreduceddatasets/MedianVolV2.csv\")\n# median_vol.index.name = \"stock_id\"\n# median_vol = median_vol[[\"overall_medvol\",\"first5min_medvol\",\"last5min_medvol\"]]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2023-12-19T13:50:30.011039Z","iopub.execute_input":"2023-12-19T13:50:30.011494Z","iopub.status.idle":"2023-12-19T13:50:32.358229Z","shell.execute_reply.started":"2023-12-19T13:50:30.011462Z","shell.execute_reply":"2023-12-19T13:50:32.357303Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"change_dtypes = {\"stock_id\": np.uint8,\n                \"date_id\": np.uint16, \n                \"seconds_in_bucket\": np.uint16,\n                \"imbalance_buy_sell_flag\": np.int8,\n                \"time_id\": np.uint16}\n\ntrain_df = pd.read_csv(PATH + \"train.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n# test_df = pd.read_csv(PATH + \"example_test_files/test.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n\ntrain_df.dropna(subset=[\"target\"], inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ndel change_dtypes","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:32.359431Z","iopub.execute_input":"2023-12-19T13:50:32.359968Z","iopub.status.idle":"2023-12-19T13:50:49.751158Z","shell.execute_reply.started":"2023-12-19T13:50:32.359928Z","shell.execute_reply":"2023-12-19T13:50:49.749120Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df[\"far_price\"] = train_df[\"far_price\"][train_df[\"far_price\"].isna() & train_df[\"date_id\"] < 300].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:49.753901Z","iopub.execute_input":"2023-12-19T13:50:49.754359Z","iopub.status.idle":"2023-12-19T13:50:49.834793Z","shell.execute_reply.started":"2023-12-19T13:50:49.754329Z","shell.execute_reply":"2023-12-19T13:50:49.832864Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df[\"near_price\"] = train_df[\"near_price\"][train_df[\"near_price\"].isna() & train_df[\"date_id\"] < 300].fillna(0)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:49.839175Z","iopub.execute_input":"2023-12-19T13:50:49.840608Z","iopub.status.idle":"2023-12-19T13:50:49.893569Z","shell.execute_reply.started":"2023-12-19T13:50:49.840515Z","shell.execute_reply":"2023-12-19T13:50:49.892354Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:49.894852Z","iopub.execute_input":"2023-12-19T13:50:49.895173Z","iopub.status.idle":"2023-12-19T13:50:49.967748Z","shell.execute_reply.started":"2023-12-19T13:50:49.895146Z","shell.execute_reply":"2023-12-19T13:50:49.966994Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"stock_id                     0\ndate_id                      0\nseconds_in_bucket            0\nimbalance_size             132\nimbalance_buy_sell_flag      0\nreference_price            132\nmatched_size               132\nfar_price                    0\nnear_price                   0\nbid_price                  132\nbid_size                     0\nask_price                  132\nask_size                     0\nwap                        132\ntarget                       0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# train_df.drop(columns=[\"far_price\",\"near_price\"], inplace=True)\ntrain_df.dropna(inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:49.968615Z","iopub.execute_input":"2023-12-19T13:50:49.970380Z","iopub.status.idle":"2023-12-19T13:50:50.385198Z","shell.execute_reply.started":"2023-12-19T13:50:49.970294Z","shell.execute_reply":"2023-12-19T13:50:50.384323Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print(f\"starting memory: {start_mem}\")\n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # check if the column's data type is not \"object\" (i.e. numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                    \n                else:\n                    # check if the column's data type is a float\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float32)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float32)\n                        \n        if verbose:\n            print(f\"start memory: {start_mem}\")\n            end_mem = df.memory_usage().sum() / 1024 ** 2\n            print(f\"current memory: {end_mem}\")\n        \n        return df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:50.386366Z","iopub.execute_input":"2023-12-19T13:50:50.387781Z","iopub.status.idle":"2023-12-19T13:50:50.402293Z","shell.execute_reply.started":"2023-12-19T13:50:50.387738Z","shell.execute_reply":"2023-12-19T13:50:50.401038Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n    for i in prange(num_combinations):\n        a,b,c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n            \n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a,b,c in combinations(price, 3)]\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n    columns = [f\"{a}_{b}_{c}_imb2\" for a,b,c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:50.403716Z","iopub.execute_input":"2023-12-19T13:50:50.404404Z","iopub.status.idle":"2023-12-19T13:50:50.675050Z","shell.execute_reply.started":"2023-12-19T13:50:50.404371Z","shell.execute_reply":"2023-12-19T13:50:50.674044Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# https://www.kaggle.com/code/chinzorigtganbat/feature-generation-technical-analysis-functions\n@njit(fastmath=True)\ndef rolling_average(arr, window):\n    n = len(arr)\n    result = np.empty(n)\n    result[:window] = np.nan\n    cumsum = np.cumsum(arr)\n    \n    for i in range(window, n):\n        result[i] = (cumsum[i] - cumsum[i-window]) / window\n    return result\n\n\n@njit(parallel=True)\ndef compute_rolling_averages(df_values, window_sizes):\n    num_rows, num_features = df_values.shape\n    num_windows = len(window_sizes)\n    rolling_features = np.empty((num_rows, num_features, num_windows))\n    \n    for feature_idx in prange(num_features):\n        for window_idx, window in enumerate(window_sizes):\n            rolling_features[:, feature_idx, window_idx] = rolling_average(df_values[:, feature_idx], window)\n    return rolling_features\n\n\n@njit(parallel=True)\ndef calculate_rsi(prices, period=14):\n    rsi_values = np.zeros_like(prices)\n    \n    for col in prange(prices.shape[1]):\n        price_data = prices[:, col]\n        delta = np.zeros_like(price_data)\n        delta[1:] = price_data[1:] - price_data[:-1]\n        gain = np.where(delta > 0, delta, 0)\n        loss = np.where(delta < 0, -delta, 0)\n        \n        avg_gain = np.mean(gain[:period])\n        avg_loss = np.mean(loss[:period])\n        \n        if avg_loss != 0:\n            rs = avg_gain / avg_loss\n        else:\n            rs = 1e-9\n            \n        rsi_values[:period, col] = 100 - (100 / (1+rs))\n        \n        for i in prange(period-1, len(price_data)-1):\n            avg_gain = (avg_gain * (period - 1) + gain[i]) / period\n            avg_loss = (avg_loss * (period - 1) + loss[i]) / period\n            \n            if avg_loss != 0:\n                rs = avg_gain / avg_loss\n            else:\n                rs = 1e-9\n            \n            rsi_values[i+1, col] = 100 - (100 / (1 + rs))\n    return rsi_values\n\n\n@njit(parallel=True)\ndef calculate_macd(data, short_window=12, long_window=26, signal_window=9):\n    rows, cols = data.shape\n    macd_values = np.empty((rows, cols))\n    signal_line_values = np.empty((rows, cols))\n    histogram_values = np.empty((rows, cols))\n    \n    for i in prange(cols):\n        short_ema = np.zeros(rows)\n        long_ema = np.zeros(rows)\n        for j in range(1, rows):\n            short_ema[j] = (data[j, i] - short_ema[j - 1]) * (2 / (short_window + 1)) + short_ema[j - 1]\n            long_ema[j] = (data[j, i] - long_ema[j - 1]) * (2 / (long_window + 1)) + long_ema[j - 1]\n            \n        macd_values[:, i] = short_ema - long_ema\n        signal_line = np.zeros(rows)\n        \n        for j in range(1, rows):\n            signal_line[j] = (macd_values[j,i] - signal_line[j - 1]) * (2 / (signal_window + 1)) + signal_line[j - 1]\n            \n        signal_line_values[:, i] = signal_line\n        histogram_values[:, i] = macd_values[:, i] - signal_line\n    return macd_values, signal_line_values, histogram_values\n\n\n\n@njit(parallel=True)\ndef calculate_bband(data, window=20, num_std_dev=2):\n    num_rows, num_cols = data.shape\n    upper_bands = np.zeros_like(data)\n    lower_bands = np.zeros_like(data)\n    mid_bands = np.zeros_like(data)\n    \n    for col in prange(num_cols):\n        for i in prange(window-1, num_rows):\n            window_slice = data[i-window+1:i+1, col]\n            mid_bands[i, col] = np.mean(window_slice)\n            std_dev = np.std(window_slice)\n            upper_bands[i, col] = mid_bands[i, col] + num_std_dev * std_dev\n            lower_bands[i, col] = mid_bands[i, col] - num_std_dev * std_dev\n            \n    return upper_bands, mid_bands, lower_bands","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:50.676243Z","iopub.execute_input":"2023-12-19T13:50:50.676560Z","iopub.status.idle":"2023-12-19T13:50:50.697735Z","shell.execute_reply.started":"2023-12-19T13:50:50.676534Z","shell.execute_reply":"2023-12-19T13:50:50.696255Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def generate_ta(df):\n    # define lists of price and size-related column names\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    \n    for stock_id, values in df.groupby([\"stock_id\"])[prices]:\n        # RSI: relative strength index\n        col_rsi = [f\"rsi_{col}\" for col in values.columns]\n        rsi_values = calculate_rsi(values.values)\n        df.loc[values.index, col_rsi] = rsi_values\n        gc.collect()\n        \n        # MACD\n        macd_values, signal_line_values, histogram_values = calculate_macd(values.values)\n        col_macd = [f\"macd_{col}\" for col in values.columns]\n        col_signal = [f\"macd_sig_{col}\" for col in values.columns]\n        col_hist = [f\"macd_hist_{col}\" for col in values.columns]\n        \n        df.loc[values.index, col_macd] = macd_values\n        df.loc[values.index, col_signal] = signal_line_values\n        df.loc[values.index, col_hist] = histogram_values\n        gc.collect()\n        \n        # bollinger Bands\n        bband_upper_values, bband_mid_values, bband_lower_values = calculate_bband(values.values, window=20, num_std_dev=2)\n        col_bband_upper = [f\"bband_upper_{col}\" for col in values.columns]\n        col_bband_mid = [f\"bband_mid_{col}\" for col in values.columns]\n        col_bband_lower = [f\"bband_lower_{col}\" for col in values.columns]\n        \n        df.loc[values.index, col_bband_upper] = bband_upper_values\n        df.loc[values.index, col_bband_mid] = bband_mid_values\n        df.loc[values.index, col_bband_lower] = bband_lower_values\n        gc.collect()\n        \n    return df, [col for col in df.columns if col not in [\"date_id\",\"stock_id\"]]","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:50.699962Z","iopub.execute_input":"2023-12-19T13:50:50.700392Z","iopub.status.idle":"2023-12-19T13:50:50.719667Z","shell.execute_reply.started":"2023-12-19T13:50:50.700363Z","shell.execute_reply":"2023-12-19T13:50:50.716424Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def create_features(df, reduce_memory=True):\n#     cols_to_drop = [\"imbalance_buy_sell_flag\"]\n    prices = [\"reference_price\",\"far_price\",\"near_price\",\"ask_price\",\"bid_price\",\"wap\"]\n    sizes = [\"matched_size\",\"bid_size\",\"ask_size\",\"imbalance_size\"]\n    \n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n#     df[\"mid_price\"] = df.eval(\"(ask_price + bid_size) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size - ask_size)/(bid_size + ask_size)\")\n#     df[\"matched_imbalance\"] = df.eval(\"(imbalance_size - matched_size)/(matched_size + imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    df[\"price_spread\"] = df.eval(\"ask_price - bid_price\")\n    df[\"imbalance_ratio\"] = df.eval(\"imbalance_size / matched_size\")\n    \n    df[\"ask_volume\"] = df.eval(\"ask_size * ask_price\")\n    df[\"bid_volume\"] = df.eval(\"bid_size * bid_price\")\n    \n    df[\"ask_bid_volumes_diff\"] = df[\"ask_volume\"] - df[\"bid_volume\"]\n    \n    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"]) # size imbalance\n    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n    \n    df[\"imbalance_buy_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==1, True, False)\n    df[\"imbalance_sell_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==-1, True, False)\n    \n    # create features for pairwise price imbalances\n    # https://www.kaggle.com/code/judith007/lb-5-3393-rapids-gpu-speeds-up-feature-engineer\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n    \n    # V2 features # 아래 피쳐들 의미를 잘 모르겠땅\n    df[\"imbalance_momentum\"] = df.groupby([\"stock_id\"])[\"imbalance_size\"].diff(periods=1)/df[\"matched_size\"]    \n    df[\"spread_intensity\"] = df.groupby([\"stock_id\"])[\"price_spread\"].diff()\n    df[\"price_pressure\"] = df[\"imbalance_size\"] * (df[\"ask_price\"] - df[\"bid_price\"])\n    df[\"market_urgency\"] = df[\"price_spread\"] * df[\"liquidity_imbalance\"]\n    df[\"depth_pressure\"] = (df[\"ask_size\"] - df[\"bid_size\"]) * (df[\"far_price\"] - df[\"near_price\"])\n    \n    # V3 features\n    for col in [\"matched_size\",\"imbalance_size\",\"reference_price\",\"imbalance_buy_sell_flag\"]:\n        for window in [1,2,3,10]:      # 이거 숫자들의 의미가 뭐지?\n            df[f\"{col}_shift_{window}\"] = df.groupby(\"stock_id\")[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby(\"stock_id\")[col].pct_change(window)\n            \n    # calculate diff features for specific columns\n    for col in [\"ask_price\",\"bid_price\",\"ask_size\",\"bid_size\"]:\n        for window in [1,2,3,10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n            \n    for c in [[\"ask_price\",\"bid_price\",\"wap\",\"reference_price\"], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    \n#     df = df.drop(columns=cols_to_drop)\n    \n    if reduce_memory:\n        print(f\"Reducing memory usage... (Current: {df.memory_usage().sum() / 1024 / 1024:.1f} MB)\")\n        df = df.astype({\"stock_id\":\"uint8\",\n                       \"seconds_in_bucket\":\"uint16\",\n                       # converts all float64s to float32s\n                       **{col:\"float32\" for col in df.columns if df[col].dtype == \"float64\"}})\n        print(f\"Memory usage after reduction: {df.memory_usage().sum() / 1024 / 1024:.1f} MB\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:50.722610Z","iopub.execute_input":"2023-12-19T13:50:50.723147Z","iopub.status.idle":"2023-12-19T13:50:50.951191Z","shell.execute_reply.started":"2023-12-19T13:50:50.723097Z","shell.execute_reply":"2023-12-19T13:50:50.950075Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# from IPython.display import FileLink\ntrain_df = create_features(train_df)\ntrain_df, ta_cols = generate_ta(train_df)\nta_cols = [x for x in ta_cols if (x != \"target\")]\ntrain_df = reduce_mem_usage(train_df)\n# train_df = train_df.merge(median_vol, how=\"left\", left_on=\"stock_id\", right_index=True)\ndate_ids = train_df[\"date_id\"].values\n# del median_vol\n\nprint(train_df[\"target\"])\ny = train_df[\"target\"]\ntrain_df.drop(columns=[\"date_id\",\"stock_id\",\"target\"], inplace=True)\n\nfeat_importances = [\"seconds_in_bucket\",\"market_urgency\",\"near_price_ask_price_imb\",\\\n\"reference_price_wap_imb\",\"wap\",\"far_price_bid_price_imb\",\"reference_price_bid_price_imb\",\\\n\"ask_price_diff_2\",\"imbalance_size_shift_1\",\"far_price_near_price_imb\",\\\n\"matched_size_ask_size_imbalance_size_imb2\",\\\n\"matched_size_ret_10\",\"price_spread\",\"matched_size_shift_10\",\"bid_price_diff_1\",\"spread_intensity\",\\\n\"ask_price_bid_price_reference_price_imb2\",\"reference_price_ask_price_imb\",\\\n\"matched_size_bid_size_imbalance_size_imb2\",\n\"reference_price_ret_3\",\"far_price_ask_price_imb\",\\\n                    \"imbalance_momentum\",\"bid_price\",\"ask_price\",\\\n\"ask_price_diff_1\",\"imbalance_ratio\",\"imbalance_buy_sell_flag\",\\\n                    \"reference_price_ret_10\",\\\n                    \n\"reference_price\",\"volume\",\"bid_size_ask_size_imbalance_size_imb2\",\"imbalance_size\",\\\n\"bid_volume\",\"matched_size_shift_1\",\"matched_size_bid_size_ask_size_imb2\",\\\n                    \n\"bid_price_over_ask_price\",\"bid_price_wap_imb\",\"reference_price_shift_3\",\\\n\"reference_price_ret_1\",\"bid_size_diff_1\",\"reference_price_shift_1\",\\\n                    \"ask_size\",\"bid_price_diff_10\",\\\n\"matched_size_ret_1\",\"matched_size_shift_2\",\"matched_size_shift_3\",\\\n                    \"ask_price_diff_10\",\\\n\"ask_size_diff_1\",\"ask_volume\",\"ask_price_bid_price_wap_imb2\",\\\n                    \"reference_price_ret_2\",\"reference_price_shift_2\",\\\n\"matched_size_ret_3\",\"bid_size_diff_3\",\"matched_size_ret_2\",\\\n                    \"price_pressure\",\"bid_size_diff_2\",\\\n\"ask_size_diff_3\",\"reference_price_far_price_imb\",\\\n                    \"liquidity_imbalance\",\"bid_price_wap_reference_price_imb2\",\\\n\"bid_size_diff_10\",\"ask_size_diff_10\",\\\n                \"bid_size_over_ask_size\",\"ask_bid_volumes_diff\",\\\n\"depth_pressure\"] + ta_cols #, \"overall_medvol\",\"target\"\n\n\ntrain_df = train_df[feat_importances]\ngc.collect()\n\n# import seaborn as sns\n# sns.heatmap(np.around(train_df.corr(), 2)) #, annot=True) ","metadata":{"execution":{"iopub.status.busy":"2023-12-19T13:50:50.952654Z","iopub.execute_input":"2023-12-19T13:50:50.952976Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \u001b[1m\nEncountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'comb_indices' of function 'compute_triplet_imbalance'.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n\u001b[1m\nFile \"../../tmp/ipykernel_444/3417327308.py\", line 3:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n/opt/conda/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \u001b[1m\nEncountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'comb__indices' of function '__numba_parfor_gufunc_0x7b21a8eb9960'.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n\u001b[1m\nFile \"<string>\", line 1:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n","output_type":"stream"},{"name":"stdout","text":"Reducing memory usage... (Current: 3916.2 MB)\nMemory usage after reduction: 1978.1 MB\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # IR: Information Ratio     as a feature..넣어주긔 \n# def calculate_information_ratio(x1, x2):\n#     assert len(x1) == len(x2)\n#     interval = 495000  # 45 days","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgb_params2 = {'objective': 'mae',\n 'n_estimators': 5500,\n 'num_leaves': 128,\n 'subsample': 0.6,\n 'colsample_bytree': 0.8,\n 'learning_rate': 5e-05,\n 'max_depth': 11,\n 'n_jobs': 4,\n 'device': 'gpu',\n 'verbosity': -1,\n 'importance_type': 'gain'}\n\nlgb_params3 = {\n    \"objective\": \"mae\",\n    \"n_estimators\": 6000,\n    \"num_leaves\": 256,\n    \"subsample\": 0.6,\n    'learning_rate': 0.00871, \n    'max_depth': 11,\n    \"colsample_bytree\" : 0.8,\n    \"n_jobs\": 4,\n    \"device\": \"gpu\",\n    \"verbosity\": -1,\n    \"importance_type\": \"gain\",\n    \"random_state\": 42,\n    \"max_bin\": 247\n}\n\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = lgb.Booster(model_file='/kaggle/input/explained-singel-model-optiver/modelitos_para_despues/doblez-conjunto.txt')\nimportances = model.feature_importance(importance_type=\"gain\")\nfeature_names = model.feature_name()\n\nser = pd.Series(importances, index=feature_names)\ndel importances\nser_sorted = ser.sort_values().astype(int)\n\nimport matplotlib.pyplot as plt\nmax_num_features = 177\nfig, ax = plt.subplots(figsize=(12, 15))\nbars = ax.barh(ser_sorted.index[-max_num_features:],\n              ser_sorted[-max_num_features:])\nax.bar_label(bars, padding=5, fmt=\"{:,.0f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"important_features = ser_sorted.index[-180:]\nimportant_features = list(important_features)\n\nfeats = [x for x in important_features if (x in list(train_df.columns)) & (x != \"target\")]\ndel important_features\ntrain_df = train_df[feats]\n\nfeature_name = [i for i in train_df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = []\nscores = []\n\nnum_folds = 10\nfold_size = 480 // num_folds\n\n\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    \n    # Define the purged set ranges\n    purged_before_start = start - 2\n    purged_before_end = start + 2\n    purged_after_start = end - 2\n    purged_after_end = end + 2\n    \n    # Exclude the purged ranges from the test set\n    purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n                 ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n    \n    # Define test_indices excluding the purged set\n    test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n    train_indices = ~test_indices & ~purged_set\n    \n    df_fold_train = train_df[train_indices]\n    df_fold_train_target = y[train_indices]\n    df_fold_valid = train_df[test_indices]\n    df_fold_valid_target = y[test_indices]\n\n    for ft in df_fold_train.columns:\n        print(\"train: \", ft)\n\n    print(f\"Fold {i+1} Model Training\")\n    \n    # Train a LightGBM model for the current fold\n    lgb_model = lgb.LGBMRegressor(**lgb_params3)\n    lgb_model.fit(\n        df_fold_train[train_df.columns],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[train_df.columns], \n                   df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    # Append the model to the list\n    models.append(lgb_model)\n    # Save the model to a file\n#     model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n#     lgb_model.booster_.save_model(model_filename)\n#     print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    # Evaluate model performance on the validation set\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\"Fold {i+1} MAE: {fold_score}\")\n\n    # Free up memory by deleting fold specific variables\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023\niter_test = env.iter_test()\n\nlgb_model.fit(train_df, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = 0\nfor (test, revealed_targets, sample_predict) in iter_test:\n    sample_predict[\"target\"] = lgbmodel.predict(test.drop(\"row_id\", axis=1))\n    env.predict(sample_predict)\n    cnt += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}