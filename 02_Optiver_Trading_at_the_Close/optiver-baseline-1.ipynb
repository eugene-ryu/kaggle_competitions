{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":6591950,"sourceType":"datasetVersion","datasetId":3762419}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"###### https://www.kaggle.com/code/verracodeguacas/hoarders-ensemble","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:40:56.607123Z","iopub.execute_input":"2023-11-30T11:40:56.607571Z","iopub.status.idle":"2023-11-30T11:40:56.613678Z","shell.execute_reply.started":"2023-11-30T11:40:56.607510Z","shell.execute_reply":"2023-11-30T11:40:56.612532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport numpy as np\nimport pandas as pd \n# pd.set_option(\"display.max_columns\", 50)\n# pd.set_option(\"display.max_rows\", 50)\n\nfrom warnings import filterwarnings\nfilterwarnings(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pd.options.mode.chained_assignment = None # pd 경고문을 안 뜨게 해\n\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport joblib\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\n\nimport json\nimport optuna\nfrom itertools import combinations\n# from pprint import pprint, pformat\n# from optuna.visualization import (plot_optimization_history,\n#                                  plot_param_importances,\n#                                  plot_parallel_coordinate)\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nPATH = \"/kaggle/input/optiver-trading-at-the-close/\"\n# reduced_PATH = \"/kaggle/input/optiver-memoryreduceddatasets/\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-30T11:40:56.615703Z","iopub.execute_input":"2023-11-30T11:40:56.616017Z","iopub.status.idle":"2023-11-30T11:41:00.836138Z","shell.execute_reply.started":"2023-11-30T11:40:56.615986Z","shell.execute_reply":"2023-11-30T11:41:00.835299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"change_dtypes = {\"stock_id\": np.uint8,\n                \"date_id\": np.uint16, \n                \"seconds_in_bucket\": np.uint16,\n                \"imbalance_buy_sell_flag\": np.int8,\n                \"time_id\": np.uint16}\n\ntrain_df = pd.read_csv(PATH + \"train.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n# test_df = pd.read_csv(PATH + \"example_test_files/test.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n\ntrain_df.dropna(subset=[\"target\"], inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ndel change_dtypes","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:41:00.837317Z","iopub.execute_input":"2023-11-30T11:41:00.837625Z","iopub.status.idle":"2023-11-30T11:41:19.359487Z","shell.execute_reply.started":"2023-11-30T11:41:00.837593Z","shell.execute_reply":"2023-11-30T11:41:19.358507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:41:19.360724Z","iopub.execute_input":"2023-11-30T11:41:19.361013Z","iopub.status.idle":"2023-11-30T11:41:19.387130Z","shell.execute_reply.started":"2023-11-30T11:41:19.360987Z","shell.execute_reply":"2023-11-30T11:41:19.386240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print(f\"starting memory: {start_mem}\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # check if the column's data type is not \"object\" (i.e. numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                    \n                else:\n                    # check if the column's data type is a float\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float32)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float32)\n                        \n        if verbose:\n            print(f\"start memory: {start_mem}\")\n            end_mem = df.memory_usage().sum() / 1024 ** 2\n            print(f\"current memory: {end_mem}\")\n        \n        return df","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:41:19.389380Z","iopub.execute_input":"2023-11-30T11:41:19.389661Z","iopub.status.idle":"2023-11-30T11:41:19.512466Z","shell.execute_reply.started":"2023-11-30T11:41:19.389637Z","shell.execute_reply":"2023-11-30T11:41:19.511433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n    for i in prange(num_combinations):\n        a,b,c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n            \n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a,b,c in combinations(price, 3)]\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n    columns = [f\"{a}_{b}_{c}_imb2\" for a,b,c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:41:19.513761Z","iopub.execute_input":"2023-11-30T11:41:19.514099Z","iopub.status.idle":"2023-11-30T11:41:20.149065Z","shell.execute_reply.started":"2023-11-30T11:41:19.514066Z","shell.execute_reply":"2023-11-30T11:41:20.148288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_features(df, reduce_memory=True):\n    cols_to_drop = [\"imbalance_buy_sell_flag\"]\n    prices = [\"reference_price\",\"far_price\",\"near_price\",\"ask_price\",\"bid_price\",\"wap\"]\n    sizes = [\"matched_size\",\"bid_size\",\"ask_size\",\"imbalance_size\"]\n    \n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_size) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size - ask_size)/(bid_size + ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size - matched_size)/(matched_size + imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n#     df[\"price_spread\"] = df.eval(\"ask_price - bid_price\")\n#     df[\"imbalance_ratio\"] = df.eval(\"imbalance_size / matched_size\")\n    \n#     df[\"ask_volume\"] = df.eval(\"ask_size * ask_price\")\n#     df[\"bid_volume\"] = df.eval(\"bid_size * bid_price\")\n    \n#     df[\"ask_bid_volumes_diff\"] = df[\"ask_volume\"] - df[\"bid_volume\"]\n    \n    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"]) # size imbalance\n    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n    \n#     df[\"imbalance_buy_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==1, True, False)\n#     df[\"imbalance_sell_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==-1, True, False)\n    \n    # create features for pairwise price imbalances\n    # https://www.kaggle.com/code/judith007/lb-5-3393-rapids-gpu-speeds-up-feature-engineer\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n    \n    # V2 features # 아래 피쳐들 의미를 잘 모르겠땅\n    df[\"imbalance_momentum\"] = df.groupby([\"stock_id\"])[\"imbalance_size\"].diff(periods=1)/df[\"matched_size\"]    \n#     df[\"spread_intensity\"] = df.groupby([\"stock_id\"])[\"price_spread\"].diff()\n    df[\"price_pressure\"] = df[\"imbalance_size\"] * (df[\"ask_price\"] - df[\"bid_price\"])\n#     df[\"market_urgency\"] = df[\"price_spread\"] * df[\"imbalance_size1\"]\n    df[\"depth_pressure\"] = (df[\"ask_size\"] - df[\"bid_size\"]) * (df[\"far_price\"] - df[\"near_price\"])\n    \n    # V3 features\n    for col in [\"matched_size\",\"imbalance_size\",\"reference_price\",\"imbalance_buy_sell_flag\"]:\n        for window in [1,2,3,10]:      # 이거 숫자들의 의미가 뭐지?\n            df[f\"{col}_shift_{window}\"] = df.groupby(\"stock_id\")[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby(\"stock_id\")[col].pct_change(window)\n            \n    # calculate diff features for specific columns\n    for col in [\"ask_price\",\"bid_price\",\"ask_size\",\"bid_size\"]:\n        for window in [1,2,3,10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n            \n    for c in [[\"ask_price\",\"bid_price\",\"wap\",\"reference_price\"], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    \n    df = df.drop(columns=cols_to_drop)\n    \n    if reduce_memory:\n        print(f\"Reducing memory usage... (Current: {df.memory_usage().sum() / 1024 / 1024:.1f} MB)\")\n        df = df.astype({\"stock_id\":\"uint8\",\n                       \"seconds_in_bucket\":\"uint16\",\n                       # converts all float64s to float32s\n                       **{col:\"float32\" for col in df.columns if df[col].dtype == \"float64\"}})\n        print(f\"Memory usage after reduction: {df.memory_usage().sum() / 1024 / 1024:.1f} MB\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:41:41.975484Z","iopub.execute_input":"2023-11-30T11:41:41.976179Z","iopub.status.idle":"2023-11-30T11:41:41.996272Z","shell.execute_reply.started":"2023-11-30T11:41:41.976140Z","shell.execute_reply":"2023-11-30T11:41:41.995091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\ntrain_df = create_features(train_df)\ntrain_df = reduce_mem_usage(train_df)\nprint(train_df.shape)\n#     os.chdir(\"/kaggle/working/\")\n#     FileLink(\"train.feather\")\n\ny = train_df[\"target\"]\ntrain_df.drop(columns=[\"target\",\"date_id\"], inplace=True)\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:41:42.198199Z","iopub.execute_input":"2023-11-30T11:41:42.198616Z","iopub.status.idle":"2023-11-30T11:42:08.976572Z","shell.execute_reply.started":"2023-11-30T11:41:42.198578Z","shell.execute_reply":"2023-11-30T11:42:08.975475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for_drops = []\nfor col,d in zip(train_df.columns, train_df.isnull().sum()):\n#     print(col, \" :: \", d)\n    if (d/5237892) > 0.5:\n        for_drops.append(col)","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:08.978466Z","iopub.execute_input":"2023-11-30T11:42:08.978831Z","iopub.status.idle":"2023-11-30T11:42:09.625959Z","shell.execute_reply.started":"2023-11-30T11:42:08.978802Z","shell.execute_reply":"2023-11-30T11:42:09.624929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.drop(columns=for_drops, inplace=True)\n# print(train_df.shape)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:09.627482Z","iopub.execute_input":"2023-11-30T11:42:09.627852Z","iopub.status.idle":"2023-11-30T11:42:11.392447Z","shell.execute_reply.started":"2023-11-30T11:42:09.627819Z","shell.execute_reply":"2023-11-30T11:42:11.391473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cross_validate(model, X, y, cv):\n    scores = np.zeros(cv.n_splits)\n    \n    print(f\"Starting evaluation..\")\n    print(\"=\"*30)\n    for i, (train_index, test_index) in enumerate(cv.split(X)):\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=False, test_size=0.1)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(25, verbose=False)])\n        \n        y_pred = model.predict(X_test)\n        scores[i] = mean_absolute_error(y_pred, y_test)\n        print(f\"Fold {i+1}: {scores[i]:.4f}\")\n        \n    print(\"-\"*30)\n    print(f\"Average MAE = {scores.mean():.4f} ± {scores.std():.2f}\")\n    print(\"=\"*30)\n    \n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:11.395048Z","iopub.execute_input":"2023-11-30T11:42:11.395348Z","iopub.status.idle":"2023-11-30T11:42:11.403853Z","shell.execute_reply.started":"2023-11-30T11:42:11.395312Z","shell.execute_reply":"2023-11-30T11:42:11.402897Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### optimizing Optuna\ndef objective(trial):\n    params = {\"random_seed\": 123,\n             \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000),\n             \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n             \"max_depth\": trial.suggest_int(\"max_depth\", 1, 12),\n             \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),\n             \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n             \"subsample_freq\": trial.suggest_categorical(\"subsample_freq\", [0,1]),\n             \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n             \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 5e-1, 1.0, log=True)}\n#               \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.07, 0.12, log=False),\n#               \"num_leaves\": num_leaves}\n    \n    model = lgb.LGBMRegressor(**params)\n    model.fit(train_df, y)\n    y_pred = model.predict(train_df)\n    score = mean_absolute_error(y, y_pred)\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:11.404987Z","iopub.execute_input":"2023-11-30T11:42:11.405327Z","iopub.status.idle":"2023-11-30T11:42:11.421498Z","shell.execute_reply.started":"2023-11-30T11:42:11.405297Z","shell.execute_reply":"2023-11-30T11:42:11.420597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run_optimization(objective, n_trials=100, n_jobs=1, best_trial=None):\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    study = optuna.create_study(direction=\"minimize\")\n    \n    if best_trial is not None:\n        print(\"Enqueuing previous best trial...\")\n        study.enqueue_trial(best_trial)\n    \n    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs, show_progress_bar=True)\n    print(study.best_params)\n    \n    print(f\"Num of finished trials: {len(study.trials)}\")\n    print(f\"Best MAE: {study.best_value:.4f}\")\n    \n    print(\"Params\")\n    print(\"=\" * 10)\n    print(study.best_params)\n    with open(\"/kaggle/working/best_params.json\", \"w\") as f:\n        json.dump(study.best_params, f)\n        os.chdir(\"/kaggle/working/\")\n        FileLink(\"best_params.json\")\n    return study","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:11.422693Z","iopub.execute_input":"2023-11-30T11:42:11.423325Z","iopub.status.idle":"2023-11-30T11:42:11.433559Z","shell.execute_reply.started":"2023-11-30T11:42:11.423291Z","shell.execute_reply":"2023-11-30T11:42:11.432811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"LOGGING_LEVELS = [\"DEBUG\",\"WARNING\",\"INFO\",\"SUCCESS\",\"WARNING\"]\ndef get_objective_function(cv=None, logging_level=\"info\"):\n    \"\"\"Returns the objective function for optuna.\"\"\"\n    # configure logging level\n    if logging_level.upper() not in LOGGING_LEVELS:\n        raise ValueError(f\"Expected logging_level to be one of {LOGGING_LEVELS}, but got '{logging_level}' instead.\")\n    handler = {\"sink\": sys.stdout, \"level\": logging_level.upper()}\n        \n    def optimize_lgbm(trial):\n        \"\"\"Optimizes a LGBMRegressor with cross-validation.\"\"\"\n        max_depth = 18\n        param_space = {\"boosting_type\":\"gbdt\",\n            \"objective\": \"mae\",\n            \"subsample\": 0.6,\n#             \"num_leaves\": 300, #30,\n            \"n_estimators\": 8000, # 700,\n            \"min_child_samples\": 20,\n#             \"reg_alpha\": 0.02,\n#             \"reg_lambda\": 0.01,\n            \"learning_rate\": 0.087,\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, int((2**max_depth) * 0.75)),\n            \"max_depth\": max_depth\n        }\n        model = lgb.LGBMRegressor(**param_space)    \n        scores = cross_validate(model, train_df, y, cv=cv)\n        return scores.mean()\n    return optimize_lgbm","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:11.434631Z","iopub.execute_input":"2023-11-30T11:42:11.434958Z","iopub.status.idle":"2023-11-30T11:42:11.448750Z","shell.execute_reply.started":"2023-11-30T11:42:11.434932Z","shell.execute_reply":"2023-11-30T11:42:11.447909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lgbm_best_params = {\"boosting_type\":\"gbdt\",\n            \"objective\": \"mae\",\n            \"subsample\": 0.5,  # 0.6\n            \"num_leaves\": 256, #30,\n            \"n_estimators\": 8000, # 700,\n            \"min_child_samples\": 20,\n#             \"reg_alpha\": 0.02,\n#             \"reg_lambda\": 0.01,\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 256,\n            \"max_depth\": 18}\n\nlgbm_best_params2 = {'objective': 'mae', \n                     'random_state': 1020, \n                     'max_bin': 256, \n                     'boosting_type': 'gbdt', \n                     'learning_rate': 0.015, \n                     'max_depth': 12, \n                     'n_estimators': 1400, \n                     'num_leaves': 300, \n                     'reg_alpha': 0.005, \n                     'reg_lambda': 0.001, \n                     'colsample_bytree': 0.6, \n                     'subsample': 0.0871, \n                     'min_child_samples': 128}","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:11.449936Z","iopub.execute_input":"2023-11-30T11:42:11.450214Z","iopub.status.idle":"2023-11-30T11:42:11.459488Z","shell.execute_reply.started":"2023-11-30T11:42:11.450190Z","shell.execute_reply":"2023-11-30T11:42:11.458589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"run_lgbm_optimization = False\nn_trials = 10\nlogging_level = \"info\"\ncv = TimeSeriesSplit(n_splits=3)  # 5\n\nreuse_best_trial = False\nbest_trial = lgbm_best_params\n\nimport pickle\nfrom IPython.display import clear_output\nif run_lgbm_optimization:\n    clear_output(wait=True)\n    objective = get_objective_function(cv=cv, logging_level=logging_level)\n    study = run_optimization(objective, n_trials=n_trials, n_jobs=1, \n                            best_trial=best_trial)\n    lgbm_best_params = study.best_params\n    \n    best_trial = lgbm_best_params if reuse_best_trial else None\n    print(\"best params:\", study.best_params)\n    \n    if n_trials > 1:\n        plot_param_importances(study).show()\n        plot_parallel_coordinate(study, params=[\"max_depth\",\"num_leaves\",\n                                               \"learning_rate\",\"min_split_gain\",\n                                               \"min_child_samples\"]).show()\n\n\ndef save_model(model, name: str):\n    \"\"\"Saves the LGBM model to disk in {name}.txt and {name}.pkl format.\n    \n    Load the pickled model with\n    ````\n    with open(\"{name}.pkl\", \"rb\") as f:\n         model = pickle.load(f)\n    ```     \n    \"\"\"\n    model.booster_.save_model(f\"{name}.txt\") # type lgb.basic.Booster\n\n    # and the model with pickle\n    with open(f\"/kaggle/working/{name}.pkl\", \"wb\") as f:\n        pickle.dump(model, f)\n\n        \n# _ = cross_validate(\n#     model=LGBMRegressor(**lgbm_best_params),\n#     X=train_df,\n#     y=y,\n#     cv=TimeSeriesSplit(n_splits=5))      \n        \n    \nrng = np.random.default_rng()\nseeds = rng.integers(low=0, high=1000, size=3)\nfor i, seed in enumerate(seeds):\n    print(f\"Fitting model {i} with seed={seed}\")\n    X_train, X_test, y_train, y_test = train_test_split(train_df, y, shuffle=False, test_size=0.2)\n    model = lgb.LGBMRegressor(**lgbm_best_params)\n    model.set_params(random_state=seed)\n\n    callbacks = [\n        lgb.log_evaluation(period=100),\n        lgb.early_stopping(100, verbose=True)\n    ]\n    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=callbacks)\n    mean_absolute_error(y_test, model.predict(X_test, num_iteration=model.best_iteration_))\n    save_model(model, name=f\"model-{i}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-30T11:42:11.460692Z","iopub.execute_input":"2023-11-30T11:42:11.460958Z","iopub.status.idle":"2023-11-30T12:01:47.595213Z","shell.execute_reply.started":"2023-11-30T11:42:11.460935Z","shell.execute_reply":"2023-11-30T12:01:47.594389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optiver2023\nenv = optiver2023.make_env()\niter_test = env.iter_test()\n\nmodel.fit(X_train, y_train)\n\ncnt = 0\nfor (test, revealed_targets, sample_predict) in iter_test:\n    sample_predict[\"target\"] = model.predict(test.drop(\"row_id\", axis=1))\n    env.predict(sample_predict)\n    cnt += 1","metadata":{"execution":{"iopub.status.busy":"2023-11-30T12:01:47.598074Z","iopub.execute_input":"2023-11-30T12:01:47.598844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}}]}