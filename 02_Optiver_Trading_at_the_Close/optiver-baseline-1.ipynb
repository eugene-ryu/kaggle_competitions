{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":57891,"databundleVersionId":7056235,"sourceType":"competition"},{"sourceId":6591950,"sourceType":"datasetVersion","datasetId":3762419}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"###### https://www.kaggle.com/code/verracodeguacas/hoarders-ensemble","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:33.216627Z","iopub.execute_input":"2023-12-07T14:59:33.217273Z","iopub.status.idle":"2023-12-07T14:59:33.222789Z","shell.execute_reply.started":"2023-12-07T14:59:33.217232Z","shell.execute_reply":"2023-12-07T14:59:33.221741Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport sys\nimport numpy as np\nimport pandas as pd \n# pd.set_option(\"display.max_columns\", 50)\n# pd.set_option(\"display.max_rows\", 50)\n\nfrom warnings import filterwarnings\nfilterwarnings(action=\"ignore\", category=pd.errors.PerformanceWarning)\n# pd.options.mode.chained_assignment = None # pd 경고문을 안 뜨게 해\n\n# import seaborn as sns\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\nimport joblib\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\n\nimport json\nimport optuna\nfrom itertools import combinations\n# from pprint import pprint, pformat\n# from optuna.visualization import (plot_optimization_history,\n#                                  plot_param_importances,\n#                                  plot_parallel_coordinate)\n\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \nPATH = \"/kaggle/input/optiver-trading-at-the-close/\"\nmedian_vol = pd.read_csv(\"/kaggle/input/optiver-memoryreduceddatasets/MedianVolV2.csv\")\nmedian_vol.index.name = \"stock_id\"\nmedian_vol = median_vol[[\"overall_medvol\",\"first5min_medvol\",\"last5min_medvol\"]]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-07T14:59:33.224245Z","iopub.execute_input":"2023-12-07T14:59:33.224567Z","iopub.status.idle":"2023-12-07T14:59:35.671825Z","shell.execute_reply.started":"2023-12-07T14:59:33.224537Z","shell.execute_reply":"2023-12-07T14:59:35.670666Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"change_dtypes = {\"stock_id\": np.uint8,\n                \"date_id\": np.uint16, \n                \"seconds_in_bucket\": np.uint16,\n                \"imbalance_buy_sell_flag\": np.int8,\n                \"time_id\": np.uint16}\n\ntrain_df = pd.read_csv(PATH + \"train.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n# test_df = pd.read_csv(PATH + \"example_test_files/test.csv\", dtype=change_dtypes).drop(columns=[\"row_id\",\"time_id\"], axis=1)\n\ntrain_df.dropna(subset=[\"target\"], inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ndel change_dtypes","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:35.673295Z","iopub.execute_input":"2023-12-07T14:59:35.673701Z","iopub.status.idle":"2023-12-07T14:59:48.531553Z","shell.execute_reply.started":"2023-12-07T14:59:35.673664Z","shell.execute_reply":"2023-12-07T14:59:48.530224Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:48.534280Z","iopub.execute_input":"2023-12-07T14:59:48.534599Z","iopub.status.idle":"2023-12-07T14:59:48.560912Z","shell.execute_reply.started":"2023-12-07T14:59:48.534572Z","shell.execute_reply":"2023-12-07T14:59:48.560036Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n0         0        0                  0      3180602.69   \n1         1        0                  0       166603.91   \n2         2        0                  0       302879.87   \n3         3        0                  0     11917682.27   \n4         4        0                  0       447549.96   \n\n   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n0                        1         0.999812   13380276.64        NaN   \n1                       -1         0.999896    1642214.25        NaN   \n2                       -1         0.999561    1819368.03        NaN   \n3                       -1         1.000171   18389745.62        NaN   \n4                       -1         0.999532   17860614.95        NaN   \n\n   near_price  bid_price  bid_size  ask_price   ask_size  wap    target  \n0         NaN   0.999812  60651.50   1.000026    8493.03  1.0 -3.029704  \n1         NaN   0.999896   3233.04   1.000660   20605.09  1.0 -5.519986  \n2         NaN   0.999403  37956.00   1.000298   18995.00  1.0 -8.389950  \n3         NaN   0.999999   2324.90   1.000214  479032.40  1.0 -4.010200  \n4         NaN   0.999394  16485.54   1.000016     434.10  1.0 -7.349849  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>stock_id</th>\n      <th>date_id</th>\n      <th>seconds_in_bucket</th>\n      <th>imbalance_size</th>\n      <th>imbalance_buy_sell_flag</th>\n      <th>reference_price</th>\n      <th>matched_size</th>\n      <th>far_price</th>\n      <th>near_price</th>\n      <th>bid_price</th>\n      <th>bid_size</th>\n      <th>ask_price</th>\n      <th>ask_size</th>\n      <th>wap</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3180602.69</td>\n      <td>1</td>\n      <td>0.999812</td>\n      <td>13380276.64</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999812</td>\n      <td>60651.50</td>\n      <td>1.000026</td>\n      <td>8493.03</td>\n      <td>1.0</td>\n      <td>-3.029704</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>166603.91</td>\n      <td>-1</td>\n      <td>0.999896</td>\n      <td>1642214.25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999896</td>\n      <td>3233.04</td>\n      <td>1.000660</td>\n      <td>20605.09</td>\n      <td>1.0</td>\n      <td>-5.519986</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>302879.87</td>\n      <td>-1</td>\n      <td>0.999561</td>\n      <td>1819368.03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999403</td>\n      <td>37956.00</td>\n      <td>1.000298</td>\n      <td>18995.00</td>\n      <td>1.0</td>\n      <td>-8.389950</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>11917682.27</td>\n      <td>-1</td>\n      <td>1.000171</td>\n      <td>18389745.62</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999999</td>\n      <td>2324.90</td>\n      <td>1.000214</td>\n      <td>479032.40</td>\n      <td>1.0</td>\n      <td>-4.010200</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>447549.96</td>\n      <td>-1</td>\n      <td>0.999532</td>\n      <td>17860614.95</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999394</td>\n      <td>16485.54</td>\n      <td>1.000016</td>\n      <td>434.10</td>\n      <td>1.0</td>\n      <td>-7.349849</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    start_mem = df.memory_usage().sum() / 1024 ** 2\n    print(f\"starting memory: {start_mem}\")\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        # check if the column's data type is not \"object\" (i.e. numeric)\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            # check if the column's data type is an integer\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                    \n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                    \n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n                    \n                else:\n                    # check if the column's data type is a float\n                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                        df[col] = df[col].astype(np.float32)\n                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                        df[col] = df[col].astype(np.float32)\n                    else:\n                        df[col] = df[col].astype(np.float32)\n                        \n        if verbose:\n            print(f\"start memory: {start_mem}\")\n            end_mem = df.memory_usage().sum() / 1024 ** 2\n            print(f\"current memory: {end_mem}\")\n        \n        return df","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:48.561935Z","iopub.execute_input":"2023-12-07T14:59:48.562228Z","iopub.status.idle":"2023-12-07T14:59:48.575142Z","shell.execute_reply.started":"2023-12-07T14:59:48.562203Z","shell.execute_reply":"2023-12-07T14:59:48.574133Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from numba import njit, prange\n\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n    for i in prange(num_combinations):\n        a,b,c = comb_indices[i]\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n            \n    return imbalance_features\n\n\ndef calculate_triplet_imbalance_numba(price, df):\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a,b,c in combinations(price, 3)]\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n    columns = [f\"{a}_{b}_{c}_imb2\" for a,b,c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n    return features","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:48.576495Z","iopub.execute_input":"2023-12-07T14:59:48.576767Z","iopub.status.idle":"2023-12-07T14:59:48.837871Z","shell.execute_reply.started":"2023-12-07T14:59:48.576743Z","shell.execute_reply":"2023-12-07T14:59:48.836966Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def create_features(df, reduce_memory=True):\n#     cols_to_drop = [\"imbalance_buy_sell_flag\"]\n    prices = [\"reference_price\",\"far_price\",\"near_price\",\"ask_price\",\"bid_price\",\"wap\"]\n    sizes = [\"matched_size\",\"bid_size\",\"ask_size\",\"imbalance_size\"]\n    \n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n#     df[\"mid_price\"] = df.eval(\"(ask_price + bid_size) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size - ask_size)/(bid_size + ask_size)\")\n#     df[\"matched_imbalance\"] = df.eval(\"(imbalance_size - matched_size)/(matched_size + imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n    \n    df[\"price_spread\"] = df.eval(\"ask_price - bid_price\")\n    df[\"imbalance_ratio\"] = df.eval(\"imbalance_size / matched_size\")\n    \n    df[\"ask_volume\"] = df.eval(\"ask_size * ask_price\")\n    df[\"bid_volume\"] = df.eval(\"bid_size * bid_price\")\n    \n    df[\"ask_bid_volumes_diff\"] = df[\"ask_volume\"] - df[\"bid_volume\"]\n    \n    df[\"bid_size_over_ask_size\"] = df[\"bid_size\"].div(df[\"ask_size\"]) # size imbalance\n    df[\"bid_price_over_ask_price\"] = df[\"bid_price\"].div(df[\"ask_price\"])\n    \n    df[\"imbalance_buy_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==1, True, False)\n    df[\"imbalance_sell_flag\"] = np.where(df[\"imbalance_buy_sell_flag\"]==-1, True, False)\n    \n    # create features for pairwise price imbalances\n    # https://www.kaggle.com/code/judith007/lb-5-3393-rapids-gpu-speeds-up-feature-engineer\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n    \n    # V2 features # 아래 피쳐들 의미를 잘 모르겠땅\n    df[\"imbalance_momentum\"] = df.groupby([\"stock_id\"])[\"imbalance_size\"].diff(periods=1)/df[\"matched_size\"]    \n    df[\"spread_intensity\"] = df.groupby([\"stock_id\"])[\"price_spread\"].diff()\n    df[\"price_pressure\"] = df[\"imbalance_size\"] * (df[\"ask_price\"] - df[\"bid_price\"])\n    df[\"market_urgency\"] = df[\"price_spread\"] * df[\"liquidity_imbalance\"]\n    df[\"depth_pressure\"] = (df[\"ask_size\"] - df[\"bid_size\"]) * (df[\"far_price\"] - df[\"near_price\"])\n    \n    # V3 features\n    for col in [\"matched_size\",\"imbalance_size\",\"reference_price\",\"imbalance_buy_sell_flag\"]:\n        for window in [1,2,3,10]:      # 이거 숫자들의 의미가 뭐지?\n            df[f\"{col}_shift_{window}\"] = df.groupby(\"stock_id\")[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby(\"stock_id\")[col].pct_change(window)\n            \n    # calculate diff features for specific columns\n    for col in [\"ask_price\",\"bid_price\",\"ask_size\",\"bid_size\"]:\n        for window in [1,2,3,10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n            \n    for c in [[\"ask_price\",\"bid_price\",\"wap\",\"reference_price\"], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    \n#     df = df.drop(columns=cols_to_drop)\n    \n    if reduce_memory:\n        print(f\"Reducing memory usage... (Current: {df.memory_usage().sum() / 1024 / 1024:.1f} MB)\")\n        df = df.astype({\"stock_id\":\"uint8\",\n                       \"seconds_in_bucket\":\"uint16\",\n                       # converts all float64s to float32s\n                       **{col:\"float32\" for col in df.columns if df[col].dtype == \"float64\"}})\n        print(f\"Memory usage after reduction: {df.memory_usage().sum() / 1024 / 1024:.1f} MB\")\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:48.839387Z","iopub.execute_input":"2023-12-07T14:59:48.840060Z","iopub.status.idle":"2023-12-07T14:59:48.856578Z","shell.execute_reply.started":"2023-12-07T14:59:48.840025Z","shell.execute_reply":"2023-12-07T14:59:48.855555Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\ntrain_df = create_features(train_df)\ntrain_df = reduce_mem_usage(train_df)\ntrain_df = train_df.merge(median_vol, how=\"left\", left_on=\"stock_id\", right_index=True)\ndate_ids = train_df[\"date_id\"].values\ndel median_vol\nprint(train_df.shape)\n#     os.chdir(\"/kaggle/working/\")\n#     FileLink(\"train.feather\")\n\ny = train_df[\"target\"]\ntrain_df.drop(columns=[\"date_id\",\"stock_id\"], inplace=True)\n\nfeat_importances = [\"seconds_in_bucket\",\"market_urgency\",\"near_price_ask_price_imb\",\\\n\"reference_price_wap_imb\",\"wap\",\"far_price_bid_price_imb\",\"reference_price_bid_price_imb\",\\\n\"ask_price_diff_2\",\"imbalance_size_shift_1\",\"far_price_near_price_imb\",\\\n\"matched_size_ask_size_imbalance_size_imb2\",\\\n\"matched_size_ret_10\",\"price_spread\",\"matched_size_shift_10\",\"bid_price_diff_1\",\"spread_intensity\",\\\n\"ask_price_bid_price_reference_price_imb2\",\"reference_price_ask_price_imb\",\\\n\"matched_size_bid_size_imbalance_size_imb2\",\n\"reference_price_ret_3\",\"far_price_ask_price_imb\",\\\n                    \"imbalance_momentum\",\"bid_price\",\"ask_price\",\\\n\"ask_price_diff_1\",\"imbalance_ratio\",\"imbalance_buy_sell_flag\",\\\n                    \"reference_price_ret_10\",\\\n                    \n\"reference_price\",\"volume\",\"bid_size_ask_size_imbalance_size_imb2\",\"imbalance_size\",\\\n\"bid_volume\",\"matched_size_shift_1\",\"matched_size_bid_size_ask_size_imb2\",\\\n                    \n\"bid_price_over_ask_price\",\"bid_price_wap_imb\",\"reference_price_shift_3\",\\\n\"reference_price_ret_1\",\"bid_size_diff_1\",\"reference_price_shift_1\",\\\n                    \"ask_size\",\"bid_price_diff_10\",\\\n\"matched_size_ret_1\",\"matched_size_shift_2\",\"matched_size_shift_3\",\\\n                    \"ask_price_diff_10\",\\\n\"ask_size_diff_1\",\"ask_volume\",\"ask_price_bid_price_wap_imb2\",\\\n                    \"reference_price_ret_2\",\"reference_price_shift_2\",\\\n\"matched_size_ret_3\",\"bid_size_diff_3\",\"matched_size_ret_2\",\\\n                    \"price_pressure\",\"bid_size_diff_2\",\\\n\"ask_size_diff_3\",\"reference_price_far_price_imb\",\\\n                    \"liquidity_imbalance\",\"bid_price_wap_reference_price_imb2\",\\\n\"bid_size_diff_10\",\"ask_size_diff_10\",\\\n                \"bid_size_over_ask_size\",\"ask_bid_volumes_diff\",\\\n\"depth_pressure\",\"overall_medvol\",\"target\"]\n\ntrain_df = train_df[feat_importances]\n# train_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T14:59:48.858016Z","iopub.execute_input":"2023-12-07T14:59:48.858425Z","iopub.status.idle":"2023-12-07T15:00:16.566959Z","shell.execute_reply.started":"2023-12-07T14:59:48.858391Z","shell.execute_reply":"2023-12-07T15:00:16.565903Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \u001b[1m\nEncountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'comb_indices' of function 'compute_triplet_imbalance'.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n\u001b[1m\nFile \"../../tmp/ipykernel_197/3417327308.py\", line 3:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n/opt/conda/lib/python3.10/site-packages/numba/core/ir_utils.py:2149: NumbaPendingDeprecationWarning: \u001b[1m\nEncountered the use of a type that is scheduled for deprecation: type 'reflected list' found for argument 'comb__indices' of function '__numba_parfor_gufunc_0x7ad570040910'.\n\nFor more information visit https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-reflection-for-list-and-set-types\n\u001b[1m\nFile \"<string>\", line 1:\u001b[0m\n\u001b[1m<source missing, REPL/exec in use?>\u001b[0m\n\u001b[0m\n  warnings.warn(NumbaPendingDeprecationWarning(msg, loc=loc))\n","output_type":"stream"},{"name":"stdout","text":"Reducing memory usage... (Current: 3916.3 MB)\nMemory usage after reduction: 1978.1 MB\nstarting memory: 1978.1163787841797\n(5237892, 106)\n","output_type":"stream"}]},{"cell_type":"code","source":"for_drops = []\nfor col,d in zip(train_df.columns, train_df.isnull().sum()):\n#     print(col, \" :: \", d)\n    if (d/5237892) > 0.5:\n        for_drops.append(col)","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:16.568161Z","iopub.execute_input":"2023-12-07T15:00:16.568462Z","iopub.status.idle":"2023-12-07T15:00:16.926914Z","shell.execute_reply.started":"2023-12-07T15:00:16.568436Z","shell.execute_reply":"2023-12-07T15:00:16.925944Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_df.drop(columns=for_drops, inplace=True)\n# print(train_df.shape)\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:16.928055Z","iopub.execute_input":"2023-12-07T15:00:16.928366Z","iopub.status.idle":"2023-12-07T15:00:18.056369Z","shell.execute_reply.started":"2023-12-07T15:00:16.928340Z","shell.execute_reply":"2023-12-07T15:00:18.055363Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"25304"},"metadata":{}}]},{"cell_type":"code","source":"def cross_validate(model, X, y, cv):\n    scores = np.zeros(cv.n_splits)\n    \n    print(f\"Starting evaluation..\")\n    print(\"=\"*30)\n    for i, (train_index, test_index) in enumerate(cv.split(X)):\n        \n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n        \n        X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, shuffle=False, test_size=0.1)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(25, verbose=False)])\n        \n        y_pred = model.predict(X_test)\n        scores[i] = mean_absolute_error(y_pred, y_test)\n        print(f\"Fold {i+1}: {scores[i]:.4f}\")\n        \n    print(\"-\"*30)\n    print(f\"Average MAE = {scores.mean():.4f} ± {scores.std():.2f}\")\n    print(\"=\"*30)\n    \n    return scores","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:18.057760Z","iopub.execute_input":"2023-12-07T15:00:18.058107Z","iopub.status.idle":"2023-12-07T15:00:18.067007Z","shell.execute_reply.started":"2023-12-07T15:00:18.058068Z","shell.execute_reply":"2023-12-07T15:00:18.065985Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"### optimizing Optuna\ndef objective(trial):\n    params = {\"random_seed\": 123,\n             \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1000),\n             \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 100),\n             \"max_depth\": trial.suggest_int(\"max_depth\", 1, 12),\n             \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 20, 100),\n             \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n             \"subsample_freq\": trial.suggest_categorical(\"subsample_freq\", [0,1]),\n             \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0),\n             \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 5e-1, 1.0, log=True)}\n#               \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.07, 0.12, log=False),\n#               \"num_leaves\": num_leaves}\n    \n    model = lgb.LGBMRegressor(**params)\n    model.fit(train_df, y)\n    y_pred = model.predict(train_df)\n    score = mean_absolute_error(y, y_pred)\n    return score","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:18.068251Z","iopub.execute_input":"2023-12-07T15:00:18.068531Z","iopub.status.idle":"2023-12-07T15:00:18.081791Z","shell.execute_reply.started":"2023-12-07T15:00:18.068506Z","shell.execute_reply":"2023-12-07T15:00:18.080934Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def run_optimization(objective, n_trials=100, n_jobs=1, best_trial=None):\n    optuna.logging.set_verbosity(optuna.logging.WARNING)\n    study = optuna.create_study(direction=\"minimize\")\n    \n    if best_trial is not None:\n        print(\"Enqueuing previous best trial...\")\n        study.enqueue_trial(best_trial)\n    \n    study.optimize(objective, n_trials=n_trials, n_jobs=n_jobs, show_progress_bar=True)\n    print(study.best_params)\n    \n    print(f\"Num of finished trials: {len(study.trials)}\")\n    print(f\"Best MAE: {study.best_value:.4f}\")\n    \n    print(\"Params\")\n    print(\"=\" * 10)\n    print(study.best_params)\n    with open(\"/kaggle/working/best_params.json\", \"w\") as f:\n        json.dump(study.best_params, f)\n        os.chdir(\"/kaggle/working/\")\n        FileLink(\"best_params.json\")\n    return study","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:18.085211Z","iopub.execute_input":"2023-12-07T15:00:18.085584Z","iopub.status.idle":"2023-12-07T15:00:18.093638Z","shell.execute_reply.started":"2023-12-07T15:00:18.085559Z","shell.execute_reply":"2023-12-07T15:00:18.092885Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"LOGGING_LEVELS = [\"DEBUG\",\"WARNING\",\"INFO\",\"SUCCESS\",\"WARNING\"]\ndef get_objective_function(cv=None, logging_level=\"info\"):\n    \"\"\"Returns the objective function for optuna.\"\"\"\n    # configure logging level\n    if logging_level.upper() not in LOGGING_LEVELS:\n        raise ValueError(f\"Expected logging_level to be one of {LOGGING_LEVELS}, but got '{logging_level}' instead.\")\n    handler = {\"sink\": sys.stdout, \"level\": logging_level.upper()}\n        \n    def optimize_lgbm(trial):\n        \"\"\"Optimizes a LGBMRegressor with cross-validation.\"\"\"\n        max_depth = 18\n        param_space = {\"boosting_type\":\"gbdt\",\n            \"objective\": \"mae\",\n            \"subsample\": 0.6,\n#             \"num_leaves\": 300, #30,\n            \"n_estimators\": 8000, # 700,\n            \"min_child_samples\": 20,\n#             \"reg_alpha\": 0.02,\n#             \"reg_lambda\": 0.01,\n            \"learning_rate\": 0.087,\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, int((2**max_depth) * 0.75)),\n            \"max_depth\": max_depth\n        }\n        model = lgb.LGBMRegressor(**param_space)    \n        scores = cross_validate(model, train_df, y, cv=cv)\n        return scores.mean()\n    return optimize_lgbm","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:18.094866Z","iopub.execute_input":"2023-12-07T15:00:18.095128Z","iopub.status.idle":"2023-12-07T15:00:18.109576Z","shell.execute_reply.started":"2023-12-07T15:00:18.095105Z","shell.execute_reply":"2023-12-07T15:00:18.108503Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"lgbm_best_params = {\"boosting_type\":\"gbdt\",\n            \"objective\": \"mae\",\n            \"subsample\": 0.5,  # 0.6\n            \"num_leaves\": 256, #30,\n            \"n_estimators\": 8000, # 700,\n            \"min_child_samples\": 20,\n#             \"reg_alpha\": 0.02,\n#             \"reg_lambda\": 0.01,\n            \"learning_rate\": 0.05,\n            \"num_leaves\": 256,\n            \"max_depth\": 18}\n\nlgb_params = {\n        \"objective\": \"mae\",\n        \"n_estimators\": 5500,\n        \"num_leaves\": 256,\n        \"subsample\": 0.6,\n        \"colsample_bytree\": 0.6,\n        \"learning_rate\": 0.00877,\n        \"n_jobs\": 4,\n        \"device\": \"gpu\",\n        \"verbosity\": -1,\n        \"importance_type\": \"gain\",\n        \"max_depth\": 12,  # Maximum depth of the tree\n        \"min_child_samples\": 15,  # Minimum number of data points in a leaf\n        \"reg_alpha\": 0.1,  # L1 regularization term\n        \"reg_lambda\": 0.3,  # L2 regularization term\n        \"min_split_gain\": 0.2,  # Minimum loss reduction required for further partitioning\n        \"min_child_weight\": 0.001,  # Minimum sum of instance weight (hessian) in a leaf\n        \"bagging_fraction\": 0.9,  # Fraction of data to be used for training each tree\n        \"bagging_freq\": 5,  # Frequency for bagging\n        \"feature_fraction\": 0.9,  # Fraction of features to be used for training each tree\n        \"num_threads\": 4,  # Number of threads for LightGBM to use\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:18.110982Z","iopub.execute_input":"2023-12-07T15:00:18.111579Z","iopub.status.idle":"2023-12-07T15:00:18.120829Z","shell.execute_reply.started":"2023-12-07T15:00:18.111545Z","shell.execute_reply":"2023-12-07T15:00:18.120058Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# run_lgbm_optimization = False\n# n_trials = 5\n# logging_level = \"info\"\n# cv = TimeSeriesSplit(n_splits=3)  # 5\n\n# reuse_best_trial = False\n# best_trial = lgb_params\n\n# import pickle\n# from IPython.display import clear_output\n# if run_lgbm_optimization:\n#     clear_output(wait=True)\n#     objective = get_objective_function(cv=cv, logging_level=logging_level)\n#     study = run_optimization(objective, n_trials=n_trials, n_jobs=1, \n#                             best_trial=best_trial)\n#     lgbm_best_params = study.best_params if reuse_best_trial else None\n#     print(\"best params:\", study.best_params)\n    \n# model_save_path = \"/kaggle/working/models/\"\nmodels = []\nscores = []\n    \n# rng = np.random.default_rng()\n# seeds = rng.integers(low=0, high=1000, size=3)\n# for i, seed in enumerate(seeds):\n#     print(f\"Fitting model {i} with seed={seed}\")\n#     X_train, X_test, y_train, y_test = train_test_split(train_df, y, shuffle=False, test_size=0.2)\n#     model = lgb.LGBMRegressor(**lgbm_best_params)\n#     model.set_params(random_state=seed)\n\n#     callbacks = [\n#         lgb.log_evaluation(period=100),\n#         lgb.early_stopping(100, verbose=True)\n#     ]\n#     model.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=callbacks)\n#     mean_absolute_error(y_test, model.predict(X_test, num_iteration=model.best_iteration_))\n#     save_model(model, name=f\"model-{i}\")\n\nnum_folds = 5\nfold_size = 480 // num_folds\nfeature_name = [i for i in train_df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\ny = train_df[\"target\"]\ntrain_df.drop(columns=[\"target\"], inplace=True)\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    \n    # Define the purged set ranges\n    purged_before_start = start - 2\n    purged_before_end = start + 2\n    purged_after_start = end - 2\n    purged_after_end = end + 2\n    \n    # Exclude the purged ranges from the test set\n    purged_set = ((date_ids >= purged_before_start) & (date_ids <= purged_before_end)) | \\\n                 ((date_ids >= purged_after_start) & (date_ids <= purged_after_end))\n    \n    # Define test_indices excluding the purged set\n    test_indices = (date_ids >= start) & (date_ids < end) & ~purged_set\n    train_indices = ~test_indices & ~purged_set\n    \n    df_fold_train = train_df[train_indices]\n    df_fold_train_target = y[train_indices]\n    df_fold_valid = train_df[test_indices]\n    df_fold_valid_target = y[test_indices]\n#     train_df.drop([\"target\"], axis=1, inplace=True)\n\n    print(f\"Fold {i+1} Model Training\")\n    \n    # Train a LightGBM model for the current fold\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_fold_train[train_df.columns],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[train_df.columns], \n                   df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    # Append the model to the list\n    models.append(lgb_model)\n    # Save the model to a file\n#     model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n#     lgb_model.booster_.save_model(model_filename)\n#     print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    # Evaluate model performance on the validation set\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_name])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\"Fold {i+1} MAE: {fold_score}\")\n\n    # Free up memory by deleting fold specific variables\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-07T15:00:18.122047Z","iopub.execute_input":"2023-12-07T15:00:18.122471Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Fold 1 Model Training\n[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=0.6 will be ignored. Current value: bagging_fraction=0.9\n[LightGBM] [Warning] num_threads is set=4, n_jobs=4 will be ignored. Current value: num_threads=4\n[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=0.6 will be ignored. Current value: feature_fraction=0.9\n[LightGBM] [Warning] bagging_freq is set=5, subsample_freq=0 will be ignored. Current value: bagging_freq=5\nTraining until validation scores don't improve for 100 rounds\n[100]\tvalid_0's l1: 5.60958\n[200]\tvalid_0's l1: 5.58268\n[300]\tvalid_0's l1: 5.57342\n[400]\tvalid_0's l1: 5.56814\n[500]\tvalid_0's l1: 5.56418\n[600]\tvalid_0's l1: 5.56186\n[700]\tvalid_0's l1: 5.56012\n[800]\tvalid_0's l1: 5.55888\n[900]\tvalid_0's l1: 5.55773\n[1000]\tvalid_0's l1: 5.55676\n[1100]\tvalid_0's l1: 5.5561\n[1200]\tvalid_0's l1: 5.55558\n[1300]\tvalid_0's l1: 5.55519\n[1400]\tvalid_0's l1: 5.55467\n[1500]\tvalid_0's l1: 5.55425\n[1600]\tvalid_0's l1: 5.55388\n[1700]\tvalid_0's l1: 5.55372\n[1800]\tvalid_0's l1: 5.55337\n[1900]\tvalid_0's l1: 5.55298\n[2000]\tvalid_0's l1: 5.55257\n[2100]\tvalid_0's l1: 5.55239\n[2200]\tvalid_0's l1: 5.55222\n[2300]\tvalid_0's l1: 5.55204\n[2400]\tvalid_0's l1: 5.55199\n[2500]\tvalid_0's l1: 5.55191\n[2600]\tvalid_0's l1: 5.55173\n[2700]\tvalid_0's l1: 5.55157\n[2800]\tvalid_0's l1: 5.55152\n[2900]\tvalid_0's l1: 5.55135\n[3000]\tvalid_0's l1: 5.55127\n[3100]\tvalid_0's l1: 5.55124\n[3200]\tvalid_0's l1: 5.55115\n[3300]\tvalid_0's l1: 5.55109\n[3400]\tvalid_0's l1: 5.55089\n[3500]\tvalid_0's l1: 5.5508\n[3600]\tvalid_0's l1: 5.55072\n[3700]\tvalid_0's l1: 5.55071\n[3800]\tvalid_0's l1: 5.55054\n[3900]\tvalid_0's l1: 5.55054\n[4000]\tvalid_0's l1: 5.55044\n[4100]\tvalid_0's l1: 5.55039\n[4200]\tvalid_0's l1: 5.55034\n[4300]\tvalid_0's l1: 5.55028\n[4400]\tvalid_0's l1: 5.55021\n[4500]\tvalid_0's l1: 5.55017\n[4600]\tvalid_0's l1: 5.55014\n[4700]\tvalid_0's l1: 5.5501\nEarly stopping, best iteration is:\n[4617]\tvalid_0's l1: 5.55008\n","output_type":"stream"}]},{"cell_type":"code","source":"import optiver2023\niter_test = env.iter_test()\n\n# lgb_model.fit(train_df, y)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = 0\nfor (test, revealed_targets, sample_predict) in iter_test:\n    sample_predict[\"target\"] = lgbmodel.predict(test.drop(\"row_id\", axis=1))\n    env.predict(sample_predict)\n    cnt += 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}}]}